{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acb35af6-adf0-4f44-9b17-75fd2febe8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19367/4052480766.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import functools\n",
    "import os\n",
    "import pandas as pd\n",
    "import e3x\n",
    "import flax.linen as nn\n",
    "import jax\n",
    "import numpy as np\n",
    "import optax\n",
    "# Disable future warnings.\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.set_cmap('bwr')\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, grad\n",
    "from jax import vmap\n",
    "\n",
    "\n",
    "\n",
    "def prepare_datasets(key, num_train, num_valid, filename=\"esp2000.npz\"):\n",
    "    # Load the dataset.\n",
    "    dataset = np.load(filename)\n",
    "\n",
    "    for k, v in dataset.items():\n",
    "        print(k, v.shape)\n",
    "\n",
    "    dataR = dataset['R']\n",
    "    dataZ = dataset['Z']\n",
    "    dataMono = dataset['mono']\n",
    "    dataEsp = dataset[\"esp\"]\n",
    "    dataVDW = dataset[\"vdw_surface\"]\n",
    "    dataNgrid = dataset[\"n_grid\"]\n",
    "\n",
    "    # Make sure that the dataset contains enough entries.\n",
    "    num_data = len(dataR)\n",
    "    print(num_data)\n",
    "    num_draw = num_train + num_valid\n",
    "    if num_draw > num_data:\n",
    "        raise RuntimeError(\n",
    "            f'datasets only contains {num_data} points, '\n",
    "            f'requested num_train={num_train}, num_valid={num_valid}')\n",
    "\n",
    "    # Randomly draw train and validation sets from dataset.\n",
    "    choice = np.asarray(\n",
    "        jax.random.choice(key, num_data, shape=(num_draw,), replace=False))\n",
    "    train_choice = choice[:num_train]\n",
    "    valid_choice = choice[num_train:]\n",
    "\n",
    "    atomic_numbers = dataZ\n",
    "\n",
    "    # Collect and return train and validation sets.\n",
    "    train_data = dict(\n",
    "        atomic_numbers=jnp.asarray(atomic_numbers[train_choice]),\n",
    "        ngrid=jnp.array(dataNgrid[train_choice]),\n",
    "        positions=jnp.asarray(dataR[train_choice]),\n",
    "        mono=jnp.asarray(dataMono[train_choice]),\n",
    "        esp=jnp.asarray(dataEsp[train_choice]),\n",
    "        vdw_surface=jnp.asarray(dataVDW[train_choice]),\n",
    "    )\n",
    "    valid_data = dict(\n",
    "        atomic_numbers=jnp.asarray(atomic_numbers[valid_choice]),\n",
    "        positions=jnp.asarray(dataR[valid_choice]),\n",
    "        mono=jnp.asarray(dataMono[valid_choice]),\n",
    "        ngrid=jnp.array(dataNgrid[valid_choice]),\n",
    "        esp=jnp.asarray(dataEsp[valid_choice]),\n",
    "        vdw_surface=jnp.asarray(dataVDW[valid_choice]),\n",
    "    )\n",
    "    print(\"...\")\n",
    "    print(\"...\")\n",
    "    for k, v in train_data.items():\n",
    "        print(k, v.shape)\n",
    "    print(\"...\")\n",
    "    for k, v in valid_data.items():\n",
    "        print(k, v.shape)\n",
    "\n",
    "    return train_data, valid_data\n",
    "\n",
    "def prepare_batches(key, data, batch_size):\n",
    "    # Determine the number of training steps per epoch.\n",
    "    data_size = len(data['mono'])\n",
    "    steps_per_epoch = data_size // batch_size\n",
    "\n",
    "    # Draw random permutations for fetching batches from the train data.\n",
    "    perms = jax.random.permutation(key, data_size)\n",
    "    perms = perms[:steps_per_epoch * batch_size]  # Skip the last batch (if incomplete).\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "\n",
    "    # Prepare entries that are identical for each batch.\n",
    "    num_atoms = len(data['atomic_numbers'][0])\n",
    "    batch_segments = jnp.repeat(jnp.arange(batch_size), num_atoms)\n",
    "    offsets = jnp.arange(batch_size) * num_atoms\n",
    "    dst_idx, src_idx = e3x.ops.sparse_pairwise_indices(num_atoms)\n",
    "    dst_idx = (dst_idx + offsets[:, None]).reshape(-1)\n",
    "    src_idx = (src_idx + offsets[:, None]).reshape(-1)\n",
    "    # Assemble and return batches.\n",
    "    return [\n",
    "        dict(\n",
    "            mono=data[\"mono\"][perm].reshape(-1),\n",
    "            ngrid=data[\"ngrid\"][perm].reshape(-1),\n",
    "            esp=data[\"esp\"][perm],  # .reshape(-1),\n",
    "            vdw_surface=data[\"vdw_surface\"][perm],  # .reshape(-1, 3),\n",
    "            atomic_numbers=data[\"atomic_numbers\"][perm].reshape(-1),\n",
    "            positions=data['positions'][perm].reshape(-1, 3),\n",
    "            dst_idx=dst_idx,\n",
    "            src_idx=src_idx,\n",
    "            batch_segments=batch_segments,\n",
    "        )\n",
    "        for perm in perms\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MessagePassingModel(nn.Module):\n",
    "    features: int = 32\n",
    "    max_degree: int = 2\n",
    "    num_iterations: int = 3\n",
    "    num_basis_functions: int = 8\n",
    "    cutoff: float = 5.0\n",
    "    max_atomic_number: int = 17\n",
    "    n_dcm: int = 4\n",
    "\n",
    "    def mono(self, atomic_numbers, positions, dst_idx, src_idx, batch_segments,\n",
    "             batch_size):\n",
    "        # 1. Calculate displacement vectors.\n",
    "        positions_dst = e3x.ops.gather_dst(positions, dst_idx=dst_idx)\n",
    "        positions_src = e3x.ops.gather_src(positions, src_idx=src_idx)\n",
    "        displacements = positions_src - positions_dst  # Shape (num_pairs, 3).\n",
    "\n",
    "        # 2. Expand displacement vectors in basis functions.\n",
    "        basis = e3x.nn.basis(\n",
    "            # Shape (num_pairs, 1, (max_degree+1)**2, num_basis_functions).\n",
    "            displacements,\n",
    "            num=self.num_basis_functions,\n",
    "            max_degree=self.max_degree,\n",
    "            radial_fn=e3x.nn.reciprocal_bernstein,\n",
    "            cutoff_fn=functools.partial(e3x.nn.smooth_cutoff, cutoff=self.cutoff)\n",
    "        )\n",
    "\n",
    "        x = e3x.nn.Embed(num_embeddings=self.max_atomic_number + 1,\n",
    "                         features=self.features)(atomic_numbers)\n",
    "\n",
    "        # 4. Perform iterations (message-passing + atom-wise refinement).\n",
    "        for i in range(self.num_iterations):\n",
    "            # Message-pass.\n",
    "            if i == self.num_iterations - 1:  # Final iteration.\n",
    "                y = e3x.nn.MessagePass(max_degree=max_degree,\n",
    "                                       include_pseudotensors=False\n",
    "                                       )(x, basis, dst_idx=dst_idx, src_idx=src_idx)\n",
    "            else:\n",
    "                y = e3x.nn.MessagePass()(x, basis, dst_idx=dst_idx, src_idx=src_idx)\n",
    "\n",
    "            y = e3x.nn.add(x, y)\n",
    "            # Atom-wise refinement MLP.\n",
    "            y = e3x.nn.Dense(self.features)(y)\n",
    "            y = e3x.nn.silu(y)\n",
    "            y = e3x.nn.Dense(self.features)(y)\n",
    "            # Residual connection.\n",
    "            x = e3x.nn.add(x, y)\n",
    "\n",
    "\n",
    "\n",
    "        x = e3x.nn.TensorDense(\n",
    "            features=n_dcm,\n",
    "            max_degree=1,\n",
    "            include_pseudotensors=False,\n",
    "        )(x)\n",
    "        \n",
    "        atomic_mono = e3x.nn.change_max_degree_or_type(x,\n",
    "                                                       max_degree=0,\n",
    "                                                       include_pseudotensors=False)\n",
    "        element_bias = self.param('element_bias',\n",
    "                                  lambda rng, shape: jnp.zeros(shape),\n",
    "                                  (self.max_atomic_number + 1))\n",
    "        atomic_mono = nn.Dense(n_dcm, use_bias=False,\n",
    "                               kernel_init=jax.nn.initializers.zeros\n",
    "                               )(atomic_mono)\n",
    "        atomic_mono = atomic_mono.squeeze()\n",
    "        atomic_mono += element_bias[atomic_numbers][:, None]\n",
    "        \n",
    "        atomic_dipo = x[:, 1, 1:4, :]\n",
    "        atomic_dipo = e3x.nn.silu(atomic_dipo)\n",
    "        atomic_dipo = jnp.clip(atomic_dipo, a_min=-0.3, a_max=0.3)\n",
    "        atomic_dipo += positions[:, :, None]\n",
    "\n",
    "        return atomic_mono, atomic_dipo\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, atomic_numbers, positions, dst_idx, src_idx,\n",
    "                 batch_segments=None, batch_size=None):\n",
    "        if batch_segments is None:\n",
    "            batch_segments = jnp.zeros_like(atomic_numbers)\n",
    "            batch_size = 1\n",
    "\n",
    "        return self.mono(atomic_numbers, positions, dst_idx, src_idx, batch_segments,\n",
    "                         batch_size)\n",
    "\n",
    "\n",
    "\n",
    "def nan_safe_coulomb_potential(q, r):\n",
    "    potential = jnp.where(jnp.isnan(r) | (r == 0.0), 0.0, q / (r * 1.88973))\n",
    "    return potential\n",
    "\n",
    "def calc_esp(charge_positions, charge_values, grid_positions, mono):\n",
    "    chg_mask = jnp.where(mono != 0, 1.0, 0.0)\n",
    "    # Expand the grid positions and charge positions to compute all pairwise differences\n",
    "    diff = grid_positions[:, None, :] - charge_positions[None, :, :]\n",
    "    # Compute the Euclidean distance between each grid point and each charge\n",
    "    r = jnp.linalg.norm(diff, axis=-1)\n",
    "    C = nan_safe_coulomb_potential((chg_mask * charge_values)[None, :], r)\n",
    "    V = jnp.sum(C, axis=-1)\n",
    "    return V\n",
    "\n",
    "\n",
    "batched_electrostatic_potential = vmap(calc_esp, in_axes=(0, 0, 0, 0), out_axes=0)\n",
    "\n",
    "def clip_colors(c):\n",
    "    return np.clip(c, -0.015, 0.015)\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=('batch_size', 'esp_w'))\n",
    "def esp_mono_loss(dipo_prediction, mono_prediction, esp_target,\n",
    "                  vdw_surface, mono, batch_size, esp_w):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    nonzero = jnp.nonzero(mono, size=batch_size * 60)\n",
    "    l2_loss_mono = optax.l2_loss(mono_prediction.sum(axis=-1), mono)\n",
    "    mono_loss = jnp.mean(l2_loss_mono[nonzero])\n",
    "\n",
    "    d = dipo_prediction.reshape(batch_size, NATOMS, 3, n_dcm)\n",
    "    d = jnp.moveaxis(d, -1, -2)\n",
    "    d = d.reshape(batch_size, NATOMS * n_dcm, 3)\n",
    "    mono = jnp.repeat(mono.reshape(batch_size, NATOMS), n_dcm, axis=-1)\n",
    "    m = mono_prediction.reshape(batch_size, NATOMS * n_dcm)\n",
    "\n",
    "    batched_pred = batched_electrostatic_potential(d, m, vdw_surface, mono).flatten()\n",
    "    esp_target = esp_target.flatten()\n",
    "    esp_non_zero = jnp.nonzero(esp_target, size=batch_size * 3143)\n",
    "\n",
    "    l2_loss = optax.l2_loss(batched_pred, esp_target)\n",
    "    esp_loss = jnp.mean(l2_loss[esp_non_zero])\n",
    "    esp_loss = esp_loss * esp_w\n",
    "    return esp_loss + mono_loss\n",
    "\n",
    "\n",
    "def esp_mono_loss_pots(dipo_prediction, mono_prediction, esp_target,\n",
    "                       vdw_surface, mono, batch_size):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    d = dipo_prediction.reshape(batch_size, NATOMS, 3, n_dcm)\n",
    "    d = jnp.moveaxis(d, -1, -2)\n",
    "    d = d.reshape(batch_size, NATOMS * n_dcm, 3)\n",
    "    mono = jnp.repeat(mono.reshape(batch_size, NATOMS), n_dcm, axis=-1)\n",
    "    m = mono_prediction.reshape(batch_size, NATOMS * n_dcm)\n",
    "\n",
    "    batched_pred = batched_electrostatic_potential(d, m, vdw_surface, mono)\n",
    "\n",
    "    return batched_pred\n",
    "\n",
    "def esp_loss_pots(dipo_prediction, mono_prediction,\n",
    "                  esp_target, vdw_surface, mono, batch_size):\n",
    "    d = dipo_prediction.reshape(batch_size, NATOMS, 3)\n",
    "    mono = mono.reshape(batch_size, NATOMS)\n",
    "    m = mono_prediction.reshape(batch_size, NATOMS)\n",
    "    batched_pred = batched_electrostatic_potential(d, m, vdw_surface, mono)\n",
    "\n",
    "    return batched_pred\n",
    "\n",
    "def mean_absolute_error(prediction, target, batch_size):\n",
    "    nonzero = jnp.nonzero(target, size=batch_size * 60)\n",
    "    return jnp.mean(jnp.abs(prediction[nonzero] - target[nonzero]))\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit,\n",
    "                   static_argnames=('model_apply', 'optimizer_update', 'batch_size', 'esp_w'))\n",
    "def train_step(model_apply, optimizer_update, batch,\n",
    "               batch_size, opt_state, params, esp_w):\n",
    "    def loss_fn(params):\n",
    "        mono, dipo = model_apply(\n",
    "            params,\n",
    "            atomic_numbers=batch['atomic_numbers'],\n",
    "            positions=batch['positions'],\n",
    "            dst_idx=batch['dst_idx'],\n",
    "            src_idx=batch['src_idx'],\n",
    "            batch_segments=batch['batch_segments'],\n",
    "        )\n",
    "        loss = esp_mono_loss(\n",
    "            dipo_prediction=dipo,\n",
    "            mono_prediction=mono,\n",
    "            vdw_surface=batch['vdw_surface'],\n",
    "            esp_target=batch['esp'],\n",
    "            mono=batch['mono'],\n",
    "            batch_size=batch_size,\n",
    "            esp_w=esp_w\n",
    "        )\n",
    "        return loss, (mono, dipo)\n",
    "\n",
    "    (loss, (mono, dipo)), grad = jax.value_and_grad(loss_fn, has_aux=True)(params)\n",
    "    updates, opt_state = optimizer_update(grad, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=('model_apply', 'batch_size', 'esp_w'))\n",
    "def eval_step(model_apply, batch, batch_size, params, esp_w):\n",
    "    mono, dipo = model_apply(\n",
    "        params,\n",
    "        atomic_numbers=batch['atomic_numbers'],\n",
    "        positions=batch['positions'],\n",
    "        dst_idx=batch['dst_idx'],\n",
    "        src_idx=batch['src_idx'],\n",
    "        batch_segments=batch['batch_segments'],\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    loss = esp_mono_loss(\n",
    "        dipo_prediction=dipo,\n",
    "        mono_prediction=mono,\n",
    "        vdw_surface=batch['vdw_surface'],\n",
    "        esp_target=batch['esp'],\n",
    "        mono=batch['mono'],\n",
    "        batch_size=batch_size,\n",
    "        esp_w=esp_w\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_model(key, model, train_data, valid_data,\n",
    "                num_epochs, learning_rate, batch_size,\n",
    "                esp_w=1.0,\n",
    "                restart_params=None):\n",
    "    # Initialize model parameters and optimizer state.\n",
    "    key, init_key = jax.random.split(key)\n",
    "    optimizer = optax.adam(learning_rate)\n",
    "    dst_idx, src_idx = e3x.ops.sparse_pairwise_indices(\n",
    "        len(train_data['atomic_numbers'][0]))\n",
    "    params = model.init(init_key,\n",
    "                        atomic_numbers=train_data['atomic_numbers'][0],\n",
    "                        positions=train_data['positions'][0],\n",
    "                        dst_idx=dst_idx,\n",
    "                        src_idx=src_idx,\n",
    "                        )\n",
    "    if restart_params is not None:\n",
    "        params = restart_params\n",
    "\n",
    "    opt_state = optimizer.init(params)\n",
    "\n",
    "    print(\"Preparing batches\")\n",
    "    print(\"..................\")\n",
    "    # Batches for the validation set need to be prepared only once.\n",
    "    key, shuffle_key = jax.random.split(key)\n",
    "    valid_batches = prepare_batches(shuffle_key, valid_data, batch_size)\n",
    "\n",
    "    # Train for 'num_epochs' epochs.\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # Prepare batches.\n",
    "        key, shuffle_key = jax.random.split(key)\n",
    "        train_batches = prepare_batches(shuffle_key, train_data, batch_size)\n",
    "        # Loop over train batches.\n",
    "        train_loss = 0.0\n",
    "        for i, batch in enumerate(train_batches):\n",
    "            params, opt_state, loss = train_step(\n",
    "                model_apply=model.apply,\n",
    "                optimizer_update=optimizer.update,\n",
    "                batch=batch,\n",
    "                batch_size=batch_size,\n",
    "                opt_state=opt_state,\n",
    "                params=params,\n",
    "                esp_w=esp_w\n",
    "            )\n",
    "            train_loss += (loss - train_loss) / (i + 1)\n",
    "\n",
    "        # Evaluate on validation set.\n",
    "        valid_loss = 0.0\n",
    "        for i, batch in enumerate(valid_batches):\n",
    "            loss = eval_step(\n",
    "                model_apply=model.apply,\n",
    "                batch=batch,\n",
    "                batch_size=batch_size,\n",
    "                params=params,\n",
    "                esp_w=esp_w\n",
    "            )\n",
    "            valid_loss += (loss - valid_loss) / (i + 1)\n",
    "\n",
    "        # Print progress.\n",
    "        print(f\"epoch: {epoch: 3d}      train:   valid:\")\n",
    "        print(f\"    loss [a.u.]             {train_loss : 8.3e} {valid_loss : 8.3e}\")\n",
    "\n",
    "    # Return final model parameters.\n",
    "    return params, valid_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ad386fd-83db-4e74-b9ad-59623ddd0feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cuda(id=0)]\n",
      "gpu\n",
      "[cuda(id=0)]\n",
      "R (20000, 60, 3)\n",
      "Z (20000, 60)\n",
      "N (20000,)\n",
      "mono (20000, 60, 1)\n",
      "esp (20000, 3163)\n",
      "n_grid (20000,)\n",
      "vdw_surface (20000, 3163, 3)\n",
      "20000\n",
      "...\n",
      "...\n",
      "atomic_numbers (16, 60)\n",
      "ngrid (16,)\n",
      "positions (16, 60, 3)\n",
      "mono (16, 60, 1)\n",
      "esp (16, 3163)\n",
      "vdw_surface (16, 3163, 3)\n",
      "...\n",
      "atomic_numbers (16, 60)\n",
      "positions (16, 60, 3)\n",
      "mono (16, 60, 1)\n",
      "ngrid (16,)\n",
      "esp (16, 3163)\n",
      "vdw_surface (16, 3163, 3)\n",
      "Preparing batches\n",
      "..................\n",
      "epoch:   1      train:   valid:\n",
      "    loss [a.u.]              8.515e-02  1.060e-01\n",
      "epoch:   2      train:   valid:\n",
      "    loss [a.u.]              9.531e-02  8.110e-02\n",
      "epoch:   3      train:   valid:\n",
      "    loss [a.u.]              2.164e-02  8.402e-02\n",
      "epoch:   4      train:   valid:\n",
      "    loss [a.u.]              1.579e-01  9.100e-02\n",
      "epoch:   5      train:   valid:\n",
      "    loss [a.u.]              6.775e-02  1.386e-01\n",
      "epoch:   6      train:   valid:\n",
      "    loss [a.u.]              8.640e-02  1.407e-01\n",
      "epoch:   7      train:   valid:\n",
      "    loss [a.u.]              4.594e-02  9.773e-02\n",
      "epoch:   8      train:   valid:\n",
      "    loss [a.u.]              3.293e-02  9.514e-02\n",
      "epoch:   9      train:   valid:\n",
      "    loss [a.u.]              1.784e-01  9.501e-02\n",
      "epoch:  10      train:   valid:\n",
      "    loss [a.u.]              1.586e-01  8.762e-02\n",
      "epoch:  11      train:   valid:\n",
      "    loss [a.u.]              2.492e-02  8.139e-02\n",
      "epoch:  12      train:   valid:\n",
      "    loss [a.u.]              7.657e-02  7.667e-02\n",
      "epoch:  13      train:   valid:\n",
      "    loss [a.u.]              1.241e-01  7.459e-02\n",
      "epoch:  14      train:   valid:\n",
      "    loss [a.u.]              7.275e-02  7.721e-02\n",
      "epoch:  15      train:   valid:\n",
      "    loss [a.u.]              1.236e-01  8.737e-02\n",
      "epoch:  16      train:   valid:\n",
      "    loss [a.u.]              6.158e-02  9.788e-02\n",
      "epoch:  17      train:   valid:\n",
      "    loss [a.u.]              6.409e-02  9.050e-02\n",
      "epoch:  18      train:   valid:\n",
      "    loss [a.u.]              3.221e-01  7.182e-02\n",
      "epoch:  19      train:   valid:\n",
      "    loss [a.u.]              1.692e-01  6.452e-02\n",
      "epoch:  20      train:   valid:\n",
      "    loss [a.u.]              5.329e-02  6.535e-02\n",
      "\n",
      "Preparing batches\n",
      "..................\n",
      "epoch:   1      train:   valid:\n",
      "    loss [a.u.]              8.515e-02  1.060e-01\n",
      "epoch:   2      train:   valid:\n",
      "    loss [a.u.]              9.531e-02  8.110e-02\n",
      "epoch:   3      train:   valid:\n",
      "    loss [a.u.]              2.164e-02  8.402e-02\n",
      "epoch:   4      train:   valid:\n",
      "    loss [a.u.]              1.579e-01  9.100e-02\n",
      "epoch:   5      train:   valid:\n",
      "    loss [a.u.]              6.775e-02  1.386e-01\n",
      "epoch:   6      train:   valid:\n",
      "    loss [a.u.]              8.640e-02  1.407e-01\n",
      "epoch:   7      train:   valid:\n",
      "    loss [a.u.]              4.594e-02  9.773e-02\n",
      "epoch:   8      train:   valid:\n",
      "    loss [a.u.]              3.293e-02  9.514e-02\n",
      "epoch:   9      train:   valid:\n",
      "    loss [a.u.]              1.784e-01  9.501e-02\n",
      "epoch:  10      train:   valid:\n",
      "    loss [a.u.]              1.586e-01  8.762e-02\n",
      "epoch:  11      train:   valid:\n",
      "    loss [a.u.]              2.492e-02  8.139e-02\n",
      "epoch:  12      train:   valid:\n",
      "    loss [a.u.]              7.657e-02  7.667e-02\n",
      "epoch:  13      train:   valid:\n",
      "    loss [a.u.]              1.241e-01  7.459e-02\n",
      "epoch:  14      train:   valid:\n",
      "    loss [a.u.]              7.275e-02  7.721e-02\n",
      "epoch:  15      train:   valid:\n",
      "    loss [a.u.]              1.236e-01  8.737e-02\n",
      "epoch:  16      train:   valid:\n",
      "    loss [a.u.]              6.158e-02  9.788e-02\n",
      "epoch:  17      train:   valid:\n",
      "    loss [a.u.]              6.409e-02  9.050e-02\n",
      "epoch:  18      train:   valid:\n",
      "    loss [a.u.]              3.221e-01  7.182e-02\n",
      "epoch:  19      train:   valid:\n",
      "    loss [a.u.]              1.692e-01  6.452e-02\n",
      "epoch:  20      train:   valid:\n",
      "    loss [a.u.]              5.329e-02  6.535e-02\n",
      "\n",
      "Preparing batches\n",
      "..................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '.95'\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "    devices = jax.local_devices()\n",
    "    print(devices)\n",
    "    print(jax.default_backend())\n",
    "    print(jax.devices())\n",
    "    \n",
    "    NATOMS = 60\n",
    "    data_key, train_key = jax.random.split(jax.random.PRNGKey(0), 2)\n",
    "\n",
    "    \n",
    "    # Model hyperparameters.\n",
    "    features = 32\n",
    "    max_degree = 2\n",
    "    num_iterations = 3\n",
    "    num_basis_functions = 16\n",
    "    cutoff = 4.0\n",
    "    \n",
    "    n_dcm = 4\n",
    "    # Training hyperparameters.\n",
    "    learning_rate = 0.001\n",
    "    batch_size = 16\n",
    "    esp_w = 10.0\n",
    "    restart_params = None\n",
    "    num_epochs = 20\n",
    "\n",
    "    train_data, valid_data = prepare_datasets(\n",
    "        data_key,\n",
    "        2**4,\n",
    "        2**4,\n",
    "        filename=\"data/qm9-esp20000.npz\")\n",
    "\n",
    "    # Create and train model.\n",
    "    message_passing_model = MessagePassingModel(\n",
    "        features=features,\n",
    "        max_degree=max_degree,\n",
    "        num_iterations=num_iterations,\n",
    "        num_basis_functions=num_basis_functions,\n",
    "        cutoff=cutoff,\n",
    "        n_dcm=n_dcm,\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "    for epic in range(100):\n",
    "        print(f\"epic {epic}\")\n",
    "        params, val = train_model(\n",
    "            key=train_key,\n",
    "            model=message_passing_model,\n",
    "            train_data=train_data,\n",
    "            valid_data=valid_data,\n",
    "            num_epochs=num_epochs,\n",
    "            learning_rate=learning_rate * (1 ** epic),\n",
    "            batch_size=batch_size,\n",
    "            restart_params=restart_params,\n",
    "            esp_w=esp_w\n",
    "        )\n",
    "\n",
    "        # open a file, where you want to store the data\n",
    "        with open(f'dcm{n_dcm}-{esp_w}-{epic}-{val}-esp_params.pkl', 'wb') as file:\n",
    "            pickle.dump(params, file)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25723852-d5cb-450f-a72f-33bcbd1fe06c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6421e5f-9987-4a94-bbd9-3f915e24438f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaxe3xcuda11p39",
   "language": "python",
   "name": "jaxe3xcuda11p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
