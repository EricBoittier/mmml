# Hydra sweep configuration for hyperparameter tuning
# Usage: python train.py -m --config-name=sweep

defaults:
  - override /hydra/sweeper: basic
  - override /hydra/launcher: joblib

# Hydra sweeper configuration
hydra:
  sweeper:
    params:
      # Model hyperparameters
      model.features: 64,128,256
      model.num_iterations: 2,3,4
      model.cutoff: 6.0,8.0,10.0
      
      # Training hyperparameters
      training.learning_rate: 1e-5,5e-5,1e-4,5e-4
      training.esp_weight_start: 100.0,1000.0,10000.0
      
      # Random seed for multiple runs
      seed: 42,43,44
  
  launcher:
    n_jobs: -1  # Use all available cores

# Note: This will run all combinations (3*3*3*4*3*3 = 972 runs)
# To reduce, use comma-separated values or range notation
# Or use Optuna sweeper for smarter hyperparameter search

