# Optuna-based hyperparameter optimization
# Install: pip install hydra-optuna-sweeper
# Usage: python train.py -m --config-name=sweep_optuna

defaults:
  - override /hydra/sweeper: optuna
  - override /hydra/launcher: joblib

# Hydra sweeper configuration
hydra:
  sweeper:
    _target_: hydra_plugins.hydra_optuna_sweeper.optuna_sweeper.OptunaSweeper
    
    # Optimization direction (minimize validation loss)
    direction: minimize
    
    # Number of trials
    n_trials: 50
    
    # Number of parallel jobs
    n_jobs: 4
    
    # Sampler configuration
    sampler:
      _target_: optuna.samplers.TPESampler
      seed: 42
    
    # Search space
    params:
      # Model hyperparameters
      model.features: choice(64, 128, 256)
      model.num_iterations: range(2, 5)
      model.cutoff: interval(6.0, 12.0)
      model.num_basis_functions: choice(16, 32, 64)
      
      # Training hyperparameters
      training.learning_rate: interval(1e-5, 1e-3)
      training.esp_weight_start: interval(100.0, 10000.0)
      training.chg_weight_start: interval(0.1, 10.0)
  
  launcher:
    n_jobs: 1  # Optuna handles parallelization

# Base configuration
model:
  n_dcm: 4
  max_degree: 2
  include_pseudotensors: false

training:
  num_epochs: 30  # Reduced for faster trials
  batch_size: 1
  n_bootstrap: 1  # No bootstrap for hyperparameter search
  use_grad_clip: true
  grad_clip_norm: 1.0

