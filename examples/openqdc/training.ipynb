{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c920efb4-9159-4c2a-9c2b-2e5455dad79c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CudaDevice(id=0)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!export JAX_ENABLE_X64=0\n",
    "!export XLA_PYTHON_CLIENT_PREALLOCATE=true\n",
    "!export XLA_PYTHON_CLIENT_MEM_FRACTION=0.95\n",
    "!export XLA_FLAGS=\"--xla_gpu_enable_triton_gemm=true --xla_gpu_enable_triton_softmax_fusion=true\"\n",
    "\n",
    "import jax\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f51d8d6f-eee1-459e-9727-13e24d31d6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmml import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82b4b8ad-729d-4009-96c1-115e82b5f822",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "PhysNet Training Script (Memmap + JAX-accelerated graph build)\n",
    "\n",
    "Usage example:\n",
    "  python train_physnet_memmap.py \\\n",
    "    --data_path openqdc_packed_memmap \\\n",
    "    --batch_size 256 \\\n",
    "    --num_epochs 10 \\\n",
    "    --learning_rate 1e-4 \\\n",
    "    --num_atoms 128 \\\n",
    "    --graph_mode dense   # or sparse\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Iterator, Dict, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Your project imports (keep as-is)\n",
    "import e3x\n",
    "from flax.training import orbax_utils, train_state\n",
    "from mmml.physnetjax.physnetjax.models.model import EF\n",
    "from mmml.physnetjax.physnetjax.training.trainstep import train_step\n",
    "from mmml.physnetjax.physnetjax.training.evalstep import eval_step\n",
    "from mmml.physnetjax.physnetjax.training.optimizer import get_optimizer\n",
    "from mmml.physnetjax.physnetjax.restart.restart import orbax_checkpointer\n",
    "from mmml.physnetjax.physnetjax.directories import BASE_CKPT_DIR\n",
    "\n",
    "# ---------------------------\n",
    "# Prefetcher (threaded)\n",
    "# ---------------------------\n",
    "import threading, queue\n",
    "class Prefetcher:\n",
    "    def __init__(self, iterator, n_prefetch: int = 2):\n",
    "        self._it = iter(iterator)\n",
    "        self._q = queue.Queue(maxsize=n_prefetch)\n",
    "        self._done = object()\n",
    "        def _worker():\n",
    "            try:\n",
    "                for x in self._it:\n",
    "                    self._q.put(x)\n",
    "            finally:\n",
    "                self._q.put(self._done)\n",
    "        threading.Thread(target=_worker, daemon=True).start()\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        x = self._q.get()\n",
    "        if x is self._done:\n",
    "            raise StopIteration\n",
    "        return x\n",
    "\n",
    "# ===== FIX: make dense graph JIT-safe via fixed-size nonzero; ensure fixed B by drop_last =====\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "# ---------------------------\n",
    "# Packed Memmap Loader (drop_last for static B)\n",
    "# ---------------------------\n",
    "class PackedMemmapLoader:\n",
    "    \"\"\"Yields padded (B, A, ...) numpy batches; mask is from Z>0. Drops last short batch.\"\"\"\n",
    "    def __init__(self, path: str, batch_size: int, shuffle: bool=True, bucket_size: int=8192, seed: int=0):\n",
    "        from pathlib import Path\n",
    "        self.path = path\n",
    "        self.batch_size = int(batch_size)\n",
    "        self.shuffle = shuffle\n",
    "        self.bucket_size = int(bucket_size)\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "        # metadata\n",
    "        self.offsets = np.load(os.path.join(path, \"offsets.npy\"))\n",
    "        self.n_atoms = np.load(os.path.join(path, \"n_atoms.npy\"))\n",
    "        self.N = int(self.n_atoms.shape[0])\n",
    "        sumA = int(self.offsets[-1])\n",
    "\n",
    "        # packed arrays\n",
    "        self.Z_pack = np.memmap(os.path.join(path, \"Z_pack.int32\"), dtype=np.int32,  mode=\"r\", shape=(sumA,))\n",
    "        self.R_pack = np.memmap(os.path.join(path, \"R_pack.f32\"),  dtype=np.float32, mode=\"r\", shape=(sumA, 3))\n",
    "        self.F_pack = np.memmap(os.path.join(path, \"F_pack.f32\"),  dtype=np.float32, mode=\"r\", shape=(sumA, 3))\n",
    "        self.E      = np.memmap(os.path.join(path, \"E.f64\"),       dtype=np.float64, mode=\"r\", shape=(self.N,))\n",
    "        self.Qtot   = np.memmap(os.path.join(path, \"Qtot.f64\"),    dtype=np.float64, mode=\"r\", shape=(self.N,))\n",
    "\n",
    "        self.indices = np.arange(self.N, dtype=np.int64)\n",
    "\n",
    "    def _yield_indices_bucketed(self):\n",
    "        order = self.indices.copy()\n",
    "        if self.shuffle:\n",
    "            self.rng.shuffle(order)\n",
    "        # ensure total count is a multiple of batch_size (drop_last)\n",
    "        limit = (len(order) // self.batch_size) * self.batch_size\n",
    "        order = order[:limit]\n",
    "        for s in range(0, limit, self.bucket_size):\n",
    "            chunk = order[s:s+self.bucket_size]\n",
    "            # sort by size within bucket to minimize padding\n",
    "            idx = np.argsort(self.n_atoms[chunk], kind=\"mergesort\")\n",
    "            chunk = chunk[idx]\n",
    "            for b in range(0, len(chunk), self.batch_size):\n",
    "                yield chunk[b:b+self.batch_size]  # always length == batch_size\n",
    "\n",
    "    def _slice(self, k: int):\n",
    "        a0, a1 = int(self.offsets[k]), int(self.offsets[k+1])\n",
    "        return (\n",
    "            self.Z_pack[a0:a1],\n",
    "            self.R_pack[a0:a1],\n",
    "            self.F_pack[a0:a1],\n",
    "            self.E[k],\n",
    "            self.Qtot[k],\n",
    "        )\n",
    "\n",
    "    def batches(self, num_atoms: int):\n",
    "        \"\"\"Yield fixed-shape (B,A,...) numpy dicts.\"\"\"\n",
    "        A = int(num_atoms)\n",
    "        for idx in self._yield_indices_bucketed():\n",
    "            B = len(idx)                      # fixed = batch_size\n",
    "            Z = np.zeros((B, A), dtype=np.int32)\n",
    "            R = np.zeros((B, A, 3), dtype=np.float32)\n",
    "            F = np.zeros((B, A, 3), dtype=np.float32)\n",
    "            E = np.zeros((B,), dtype=np.float64)\n",
    "            Qtot = np.zeros((B,), dtype=np.float64)\n",
    "            for j, k in enumerate(idx):\n",
    "                z, r, f, e, q = self._slice(int(k))\n",
    "                a = min(z.shape[0], A)\n",
    "                Z[j, :a] = z[:a]\n",
    "                R[j, :a] = r[:a]\n",
    "                F[j, :a] = f[:a]\n",
    "                E[j] = e\n",
    "                Qtot[j] = q\n",
    "            yield {\"Z\": Z, \"R\": R, \"F\": F, \"E\": E, \"Qtot\": Qtot}\n",
    "\n",
    "# ------------------ Dense graph (JIT) with fixed-size nonzero ------------------\n",
    "@jax.jit\n",
    "def _build_graph_dense_fixed(Z: jnp.ndarray, R: jnp.ndarray, cutoff: float):\n",
    "    \"\"\"\n",
    "    Returns padded edges with fixed size Emax = B*A*(A-1).\n",
    "    Also returns edge_mask (Emax,) marking real edges.\n",
    "    \"\"\"\n",
    "    B, A = Z.shape\n",
    "    valid = (Z > 0)                                  # (B,A)\n",
    "    dR = R[:, :, None, :] - R[:, None, :, :]         # (B,A,A,3)\n",
    "    D  = jnp.linalg.norm(dR, axis=-1)                # (B,A,A)\n",
    "    adj = (D < cutoff) & (valid[:, :, None] & valid[:, None, :]) & (~jnp.eye(A, dtype=bool)[None])\n",
    "\n",
    "    # Fixed-size nonzero with padding\n",
    "    Emax = B * A * (A - 1)                           # directed edges, no self\n",
    "    b, i, j = jnp.nonzero(adj, size=Emax, fill_value=0)  # (Emax,)\n",
    "    edge_mask = adj[b, i, j]                         # (Emax,) True for real edges, False for padded\n",
    "\n",
    "    src_idx = (b * A + i).astype(jnp.int32)          # (Emax,)\n",
    "    dst_idx = (b * A + j).astype(jnp.int32)          # (Emax,)\n",
    "    batch_segments = jnp.repeat(jnp.arange(B, dtype=jnp.int32), A)  # (B*A,)\n",
    "    return src_idx, dst_idx, batch_segments, edge_mask\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import lax\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import lax\n",
    "import functools, jax, jax.numpy as jnp\n",
    "from jax import lax\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=('tile_j',))\n",
    "def _build_graph_dense_tiled_static(Z: jnp.ndarray, R: jnp.ndarray, cutoff: float, tile_j: int = 32):\n",
    "    B, A = Z.shape\n",
    "    A_pad = ((A + tile_j - 1) // tile_j) * tile_j\n",
    "    padJ  = A_pad - A\n",
    "\n",
    "    valid     = (Z > 0)\n",
    "    R_pad     = jnp.pad(R,     ((0,0),(0,padJ),(0,0)), constant_values=0.0)\n",
    "    valid_pad = jnp.pad(valid, ((0,0),(0,padJ)),       constant_values=False)\n",
    "\n",
    "    adj0 = jnp.zeros((B, A_pad, A_pad), dtype=bool)\n",
    "\n",
    "    def body_fun(t, adj):\n",
    "        j0 = t * tile_j  # dynamic int tracer\n",
    "        # slice j-block (static size = tile_j)\n",
    "        Rj = lax.dynamic_slice_in_dim(R_pad,     start_index=j0, slice_size=tile_j, axis=1)  # (B,tile_j,3)\n",
    "        Vj = lax.dynamic_slice_in_dim(valid_pad, start_index=j0, slice_size=tile_j, axis=1)  # (B,tile_j)\n",
    "\n",
    "        dR_block = R_pad[:, :, None, :] - Rj[:, None, :, :]   # (B,A_pad,tile_j,3)\n",
    "        D_block  = jnp.linalg.norm(dR_block, axis=-1)         # (B,A_pad,tile_j)\n",
    "\n",
    "        Vi = valid_pad[:, :, None]                            # (B,A_pad,1)\n",
    "        Vj_ = Vj[:, None, :]                                  # (B,1,tile_j)\n",
    "        mask_block = (D_block < cutoff) & (Vi & Vj_)          # (B,A_pad,tile_j)\n",
    "\n",
    "        # no self-edges on diagonal positions of this tile\n",
    "        i_idx = jnp.arange(A_pad)\n",
    "        j_idx = j0 + jnp.arange(tile_j)                       # static length; values traced\n",
    "        diag_mask = (i_idx[:, None] != j_idx[None, :])\n",
    "        mask_block = mask_block & diag_mask[None, :, :]\n",
    "\n",
    "        # ✨ dynamic update instead of slicing with dynamic j0\n",
    "        adj = lax.dynamic_update_slice(adj, mask_block, (0, 0, j0))\n",
    "        return adj\n",
    "\n",
    "    n_tiles = A_pad // tile_j\n",
    "    adj = lax.fori_loop(0, n_tiles, body_fun, adj0)\n",
    "\n",
    "    # fixed-size nonzero (padded)\n",
    "    Emax = B * A_pad * (A_pad - 1)\n",
    "    b, i, j   = jnp.nonzero(adj, size=Emax, fill_value=0)\n",
    "    edge_mask = adj[b, i, j]\n",
    "\n",
    "    # clamp padded rows/cols to 0 (masked anyway)\n",
    "    i_safe = jnp.where((i < A) & edge_mask, i, 0)\n",
    "    j_safe = jnp.where((j < A) & edge_mask, j, 0)\n",
    "\n",
    "    src_idx = (b * A + i_safe).astype(jnp.int32)\n",
    "    dst_idx = (b * A + j_safe).astype(jnp.int32)\n",
    "\n",
    "    batch_segments = jnp.repeat(jnp.arange(B, dtype=jnp.int32), A)\n",
    "    return src_idx, dst_idx, batch_segments, edge_mask\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=('tile_j',))\n",
    "def preprocess_batch_dense(Z, R, F, E, Qtot, cutoff, tile_j: int = 32):\n",
    "    src_idx, dst_idx, batch_segments, edge_mask = _build_graph_dense_tiled_static(Z, R, cutoff, tile_j)\n",
    "    B, A = Z.shape\n",
    "    Zf = Z.reshape(B * A); Rf = R.reshape(B * A, 3); Ff = F.reshape(B * A, 3)\n",
    "    atom_mask  = (Zf > 0)\n",
    "    batch_mask = edge_mask  # (Emax,)\n",
    "    return {\n",
    "        \"Z\": Zf, \"R\": Rf, \"F\": Ff, \"E\": E, \"Qtot\": Qtot,\n",
    "        \"atom_mask\": atom_mask,\n",
    "        \"batch_mask\": batch_mask,\n",
    "        \"batch_segments\": batch_segments,\n",
    "        \"src_idx\": src_idx, \"dst_idx\": dst_idx,\n",
    "    }\n",
    "# ------------------ Sparse graph (eager) unchanged from prior working version ------------------\n",
    "def _per_sample_sparse_eager(rs: jnp.ndarray, z: jnp.ndarray, b: int, cutoff: float):\n",
    "    A = int(z.shape[0])\n",
    "    valid_mask = (z > 0)\n",
    "    a = int(valid_mask.sum())\n",
    "    if a <= 1:\n",
    "        return jnp.array([], dtype=jnp.int32), jnp.array([], dtype=jnp.int32)\n",
    "    valid_idx = jnp.where(valid_mask)[0]          # (a,)\n",
    "    rs_v = rs[valid_idx, :]                       # (a,3)\n",
    "    diff = rs_v[:, None, :] - rs_v[None, :, :]    # (a,a,3)\n",
    "    dist = jnp.linalg.norm(diff, axis=-1)         # (a,a)\n",
    "    adj  = (dist < cutoff) & (~jnp.eye(a, dtype=bool))\n",
    "    i_loc, j_loc = jnp.where(adj)                 # (e,)\n",
    "    if i_loc.size == 0:\n",
    "        return jnp.array([], dtype=jnp.int32), jnp.array([], dtype=jnp.int32)\n",
    "    src = b * A + valid_idx[i_loc]\n",
    "    dst = b * A + valid_idx[j_loc]\n",
    "    return src.astype(jnp.int32), dst.astype(jnp.int32)\n",
    "\n",
    "def _build_graph_sparse_eager(Z: jnp.ndarray, R: jnp.ndarray, cutoff: float):\n",
    "    B, A = map(int, Z.shape)\n",
    "    src_chunks, dst_chunks = [], []\n",
    "    for b in range(B):\n",
    "        s, d = _per_sample_sparse_eager(R[b], Z[b], b, cutoff)\n",
    "        src_chunks.append(s); dst_chunks.append(d)\n",
    "    if len(src_chunks) == 0 or src_chunks[0].size == 0:\n",
    "        src_idx = jnp.array([], dtype=jnp.int32)\n",
    "        dst_idx = jnp.array([], dtype=jnp.int32)\n",
    "    else:\n",
    "        src_idx = jnp.concatenate(src_chunks, axis=0)\n",
    "        dst_idx = jnp.concatenate(dst_chunks, axis=0)\n",
    "    batch_segments = jnp.repeat(jnp.arange(B, dtype=jnp.int32), A)\n",
    "    return src_idx, dst_idx, batch_segments\n",
    "\n",
    "@jax.jit\n",
    "def _preprocess_common_masks(Z: jnp.ndarray, R: jnp.ndarray, F: jnp.ndarray, E: jnp.ndarray, Qtot: jnp.ndarray):\n",
    "    # same as _preprocess_common but kept separate to avoid confusion\n",
    "    B, A = Z.shape\n",
    "    Zf = Z.reshape(B * A)\n",
    "    Rf = R.reshape(B * A, 3)\n",
    "    Ff = F.reshape(B * A, 3)\n",
    "    atom_mask  = (Zf > 0)\n",
    "    batch_segments = jnp.repeat(jnp.arange(B, dtype=jnp.int32), A)\n",
    "    return Zf, Rf, Ff, atom_mask, batch_segments\n",
    "\n",
    "def preprocess_batch_sparse(Z, R, F, E, Qtot, cutoff):\n",
    "    src_idx, dst_idx, batch_segments = _build_graph_sparse_eager(Z, R, cutoff)\n",
    "    Zf, Rf, Ff, atom_mask, _ = _preprocess_common_masks(Z, R, F, E, Qtot)\n",
    "    # edge-wise mask of ones for the edges we actually have\n",
    "    batch_mask = jnp.ones((src_idx.shape[0],), dtype=bool)\n",
    "    return {\n",
    "        \"Z\": Zf, \"R\": Rf, \"F\": Ff, \"E\": E, \"Qtot\": Qtot,\n",
    "        \"atom_mask\": atom_mask,\n",
    "        \"batch_mask\": batch_mask,            # (E,)\n",
    "        \"batch_segments\": batch_segments,    # (B*A,)\n",
    "        \"src_idx\": src_idx, \"dst_idx\": dst_idx,\n",
    "    }\n",
    "\n",
    "# ------------------ Public dispatcher ------------------\n",
    "def preprocess_batch(Z, R, F, E, Qtot, cutoff, graph_mode: str):\n",
    "    if graph_mode == \"dense\":\n",
    "        # return preprocess_batch_dense(Z, R, F, E, Qtot, cutoff)\n",
    "        pre = preprocess_batch_dense(Z, R, F, E, Qtot, cutoff, tile_j=16)   # 8..32; 16 is a safe start\n",
    "        return pre\n",
    "    else:\n",
    "        return preprocess_batch_sparse(Z, R, F, E, Qtot, cutoff)\n",
    "\n",
    "def preprocess_batch(Z, R, F, E, Qtot, cutoff, graph_mode: str):\n",
    "    if graph_mode == \"dense\":\n",
    "        return preprocess_batch_dense(Z, R, F, E, Qtot, cutoff, tile_j=16)  # try 8–32\n",
    "    else:\n",
    "        return preprocess_batch_sparse(Z, R, F, E, Qtot, cutoff)\n",
    "\n",
    "# ------------------ Iteration helper (prefetch) ------------------\n",
    "def iter_loader(loader: PackedMemmapLoader, num_atoms: int, prefetch: int = 2):\n",
    "    import queue, threading\n",
    "    def _gen():\n",
    "        for b in loader.batches(num_atoms=num_atoms):\n",
    "            yield b\n",
    "    class _Pref:\n",
    "        def __init__(self, it, n=2):\n",
    "            self._it = iter(it); self._q = queue.Queue(maxsize=n); self._done = object()\n",
    "            def _w():\n",
    "                try:\n",
    "                    for x in self._it: self._q.put(x)\n",
    "                finally: self._q.put(self._done)\n",
    "            threading.Thread(target=_w, daemon=True).start()\n",
    "        def __iter__(self): return self\n",
    "        def __next__(self):\n",
    "            x = self._q.get()\n",
    "            if x is self._done: raise StopIteration\n",
    "            return x\n",
    "    return _Pref(_gen(), prefetch)\n",
    "\n",
    "# ------------------ Train / Validate (unchanged API) ------------------\n",
    "def train_epoch(\n",
    "    model, params, ema_params, opt_state, transform_state, optimizer,\n",
    "    loader, energy_weight: float, forces_weight: float, num_atoms: int,\n",
    "    cutoff: float, graph_mode: str,\n",
    "):\n",
    "    train_loss = 0.0; train_energy_mae = 0.0; train_forces_mae = 0.0\n",
    "    for i, batch_np in enumerate(iter_loader(loader, num_atoms=num_atoms)):\n",
    "        Z = jnp.asarray(batch_np[\"Z\"]); R = jnp.asarray(batch_np[\"R\"]); F = jnp.asarray(batch_np[\"F\"])\n",
    "        E = jnp.asarray(batch_np[\"E\"]); Qtot = jnp.asarray(batch_np[\"Qtot\"])\n",
    "        B = int(Z.shape[0])\n",
    "        pre = preprocess_batch(Z, R, F, E, Qtot, cutoff, graph_mode)\n",
    "        (params, ema_params, opt_state, transform_state,\n",
    "         loss, energy_mae, forces_mae, _) = train_step(\n",
    "            model_apply=model.apply,\n",
    "            optimizer_update=optimizer.update,\n",
    "            transform_state=transform_state,\n",
    "            batch=pre, batch_size=B,\n",
    "            energy_weight=energy_weight, forces_weight=forces_weight,\n",
    "            dipole_weight=0.0, charges_weight=0.0, opt_state=opt_state,\n",
    "            doCharges=False, params=params, ema_params=ema_params, debug=False,\n",
    "         )\n",
    "        train_loss += (loss - train_loss) / (i + 1)\n",
    "        train_energy_mae += (energy_mae - train_energy_mae) / (i + 1)\n",
    "        train_forces_mae += (forces_mae - train_forces_mae) / (i + 1)\n",
    "        if i % 50 == 0:\n",
    "            print(f\"  Batch {i}: Loss={float(loss):.6f}  E_MAE={float(energy_mae):.6f}  F_MAE={float(forces_mae):.6f}\")\n",
    "    return params, ema_params, opt_state, transform_state, float(train_loss), float(train_energy_mae), float(train_forces_mae)\n",
    "\n",
    "def validate(\n",
    "    model, ema_params, loader, energy_weight: float, forces_weight: float,\n",
    "    num_atoms: int, cutoff: float, graph_mode: str,\n",
    "):\n",
    "    valid_loss = 0.0; valid_energy_mae = 0.0; valid_forces_mae = 0.0\n",
    "    for i, batch_np in enumerate(iter_loader(loader, num_atoms=num_atoms)):\n",
    "        Z = jnp.asarray(batch_np[\"Z\"]); R = jnp.asarray(batch_np[\"R\"]); F = jnp.asarray(batch_np[\"F\"])\n",
    "        E = jnp.asarray(batch_np[\"E\"]); Qtot = jnp.asarray(batch_np[\"Qtot\"])\n",
    "        B = int(Z.shape[0])\n",
    "        pre = preprocess_batch(Z, R, F, E, Qtot, cutoff, graph_mode)\n",
    "        loss, energy_mae, forces_mae, _ = eval_step(\n",
    "            model_apply=model.apply, batch=pre, batch_size=B,\n",
    "            energy_weight=energy_weight, forces_weight=forces_weight,\n",
    "            dipole_weight=0.0, charges_weight=0.0, charges=False, params=ema_params,\n",
    "        )\n",
    "        valid_loss += (loss - valid_loss) / (i + 1)\n",
    "        valid_energy_mae += (energy_mae - valid_energy_mae) / (i + 1)\n",
    "        valid_forces_mae += (forces_mae - valid_forces_mae) / (i + 1)\n",
    "    return float(valid_loss), float(valid_energy_mae), float(valid_forces_mae)\n",
    "\n",
    "# ---------------------------\n",
    "def main(args):\n",
    "    print(\"=\"*80)\n",
    "    print(\"PhysNet Training (Memmap + JAX graph)\")\n",
    "    print(\"=\"*80)\n",
    "    for k,v in sorted(vars(args).items()):\n",
    "        print(f\"{k}: {v}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    key = jax.random.PRNGKey(args.seed)\n",
    "\n",
    "    # Build loaders and split\n",
    "    full_loader = PackedMemmapLoader(\n",
    "        args.data_path, batch_size=args.batch_size, shuffle=True, bucket_size=args.bucket_size, seed=args.seed\n",
    "    )\n",
    "    n_total = full_loader.N\n",
    "    n_valid = int(n_total * args.valid_split)\n",
    "    n_train = n_total - n_valid\n",
    "    print(f\"Total molecules: {n_total} | Train: {n_train} | Valid: {n_valid}\")\n",
    "\n",
    "    # train loader\n",
    "    train_loader = PackedMemmapLoader(\n",
    "        args.data_path, batch_size=args.batch_size, shuffle=True, bucket_size=args.bucket_size, seed=args.seed\n",
    "    )\n",
    "    train_loader.indices = train_loader.indices[:n_train]\n",
    "    train_loader.N = n_train\n",
    "\n",
    "    # valid loader\n",
    "    valid_loader = PackedMemmapLoader(\n",
    "        args.data_path, batch_size=args.batch_size, shuffle=False, bucket_size=args.bucket_size, seed=args.seed+1\n",
    "    )\n",
    "    valid_loader.indices = valid_loader.indices[n_train:]\n",
    "    valid_loader.N = n_valid\n",
    "\n",
    "    # Model\n",
    "    model = EF(\n",
    "        features=args.features,\n",
    "        max_degree=args.max_degree,\n",
    "        num_iterations=args.num_iterations,\n",
    "        num_basis_functions=args.num_basis_functions,\n",
    "        cutoff=args.cutoff,\n",
    "        max_atomic_number=118,\n",
    "        charges=False,\n",
    "        natoms=args.num_atoms,\n",
    "        total_charge=0.0,\n",
    "        n_res=args.n_res,\n",
    "        zbl=False,\n",
    "        debug=False,\n",
    "        efa=False,\n",
    "    )\n",
    "\n",
    "    # Init params (use e3x dense pair indices for the fixed A)\n",
    "    init_key = jax.random.PRNGKey(args.seed + 1)\n",
    "    dst_idx_e3x, src_idx_e3x = e3x.ops.sparse_pairwise_indices(args.num_atoms)\n",
    "    # Get a small sample (one batch) for shapes\n",
    "    sample_np = next(train_loader.batches(num_atoms=args.num_atoms))\n",
    "    params = model.init(\n",
    "        init_key,\n",
    "        atomic_numbers=jnp.asarray(sample_np[\"Z\"][0]),   # (A,)\n",
    "        positions=jnp.asarray(sample_np[\"R\"][0]),        # (A,3)\n",
    "        dst_idx=dst_idx_e3x,\n",
    "        src_idx=src_idx_e3x,\n",
    "    )\n",
    "    n_params = sum(int(x.size) for x in jax.tree_util.tree_leaves(params))\n",
    "    print(f\"Model initialized with {n_params:,} parameters\")\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer, transform, schedule_fn, optimizer_kwargs = get_optimizer(\n",
    "        learning_rate=args.learning_rate,\n",
    "        schedule_fn=\"constant\",\n",
    "        optimizer=\"amsgrad\",\n",
    "        transform=None,\n",
    "    )\n",
    "    ema_params = params\n",
    "    opt_state = optimizer.init(params)\n",
    "    transform_state = transform.init(params)\n",
    "\n",
    "    best_valid = float(\"inf\")\n",
    "    ckpt_dir = Path(args.ckpt_dir) if args.ckpt_dir else BASE_CKPT_DIR\n",
    "    save_root = ckpt_dir / args.name\n",
    "    save_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(\"\\nStarting training...\")\n",
    "    print(\"=\"*80)\n",
    "    for epoch in range(1, args.num_epochs + 1):\n",
    "        t0 = time.time()\n",
    "        print(f\"\\nEpoch {epoch}/{args.num_epochs}\")\n",
    "        print(\"-\"*80)\n",
    "\n",
    "        # Train\n",
    "        params, ema_params, opt_state, transform_state, tr_loss, tr_e_mae, tr_f_mae = train_epoch(\n",
    "            model, params, ema_params, opt_state, transform_state, optimizer,\n",
    "            loader=train_loader,\n",
    "            energy_weight=args.energy_weight,\n",
    "            forces_weight=args.forces_weight,\n",
    "            num_atoms=args.num_atoms,\n",
    "            cutoff=args.cutoff,\n",
    "            graph_mode=args.graph_mode,\n",
    "        )\n",
    "\n",
    "        # Validate\n",
    "        va_loss, va_e_mae, va_f_mae = validate(\n",
    "            model, ema_params, valid_loader,\n",
    "            energy_weight=args.energy_weight,\n",
    "            forces_weight=args.forces_weight,\n",
    "            num_atoms=args.num_atoms,\n",
    "            cutoff=args.cutoff,\n",
    "            graph_mode=args.graph_mode,\n",
    "        )\n",
    "\n",
    "        dt = time.time() - t0\n",
    "        print(f\"\\nEpoch {epoch} Results:\")\n",
    "        print(f\"  Train: Loss={tr_loss:.6f}  E_MAE={tr_e_mae:.6f}  F_MAE={tr_f_mae:.6f}\")\n",
    "        print(f\"  Valid: Loss={va_loss:.6f}  E_MAE={va_e_mae:.6f}  F_MAE={va_f_mae:.6f}\")\n",
    "        print(f\"  Time: {dt:.2f}s\")\n",
    "\n",
    "        if va_loss < best_valid:\n",
    "            best_valid = va_loss\n",
    "            print(\"  → New best validation loss. Saving checkpoint...\")\n",
    "            state = train_state.TrainState.create(apply_fn=model.apply, params=params, tx=optimizer)\n",
    "            ckpt = {\n",
    "                \"model\": state,\n",
    "                \"model_attributes\": model.return_attributes(),\n",
    "                \"transform_state\": transform_state,\n",
    "                \"ema_params\": ema_params,\n",
    "                \"params\": params,\n",
    "                \"epoch\": epoch,\n",
    "                \"opt_state\": opt_state,\n",
    "                \"best_loss\": best_valid,\n",
    "                \"train_loss\": tr_loss,\n",
    "                \"valid_loss\": va_loss,\n",
    "            }\n",
    "            save_args = orbax_utils.save_args_from_target(ckpt)\n",
    "            orbax_checkpointer.save(save_root / f\"epoch-{epoch}\", ckpt, save_args=save_args)\n",
    "\n",
    "    print(\"\\nDone.\")\n",
    "    print(f\"Best validation loss: {best_valid:.6f}\")\n",
    "    print(f\"Checkpoints at: {save_root}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36feea2d-caf3-48d2-af32-04ba33df3134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# CLI\n",
    "# ---------------------------\n",
    "def build_args():\n",
    "    import argparse\n",
    "    p = argparse.ArgumentParser(\"PhysNet Memmap Trainer (dense/sparse graph)\")\n",
    "    # Data\n",
    "    p.add_argument(\"--data_path\", type=str, required=True)\n",
    "    p.add_argument(\"--valid_split\", type=float, default=0.125)\n",
    "    # Model\n",
    "    p.add_argument(\"--features\", type=int, default=128)\n",
    "    p.add_argument(\"--max_degree\", type=int, default=0)\n",
    "    p.add_argument(\"--num_iterations\", type=int, default=5)\n",
    "    p.add_argument(\"--num_basis_functions\", type=int, default=128)\n",
    "    p.add_argument(\"--cutoff\", type=float, default=5.0)\n",
    "    p.add_argument(\"--num_atoms\", type=int, default=128)\n",
    "    p.add_argument(\"--n_res\", type=int, default=3)\n",
    "    # Train\n",
    "    p.add_argument(\"--batch_size\", type=int, default=256)\n",
    "    p.add_argument(\"--num_epochs\", type=int, default=10)\n",
    "    p.add_argument(\"--learning_rate\", type=float, default=1e-4)\n",
    "    p.add_argument(\"--energy_weight\", type=float, default=1000.0)\n",
    "    p.add_argument(\"--forces_weight\", type=float, default=0.1)\n",
    "    p.add_argument(\"--bucket_size\", type=int, default=8192)\n",
    "    p.add_argument(\"--graph_mode\", choices=[\"dense\",\"sparse\"], default=\"dense\")\n",
    "    # Checkpointing\n",
    "    p.add_argument(\"--name\", type=str, default=\"physnet_memmap\")\n",
    "    p.add_argument(\"--ckpt_dir\", type=str, default=None)\n",
    "    p.add_argument(\"--seed\", type=int, default=0)\n",
    "    return p.parse_args()\n",
    "\n",
    "# ---------------------------\n",
    "# Notebook helper\n",
    "# ---------------------------\n",
    "def make_dummy_args(**overrides):\n",
    "    defaults = {\n",
    "        \"data_path\": \"openqdc_packed_memmap\",\n",
    "        \"valid_split\": 0.125,\n",
    "        \"features\": 128,\n",
    "        \"max_degree\": 0,\n",
    "        \"num_iterations\": 5,\n",
    "        \"num_basis_functions\": 128,\n",
    "        \"cutoff\": 5.0,\n",
    "        \"num_atoms\": 128,\n",
    "        \"n_res\": 3,\n",
    "        \"batch_size\": 64,\n",
    "        \"num_epochs\": 10,\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"energy_weight\": 1000.0,\n",
    "        \"forces_weight\": 0.1,\n",
    "        \"bucket_size\": 8192,\n",
    "        \"graph_mode\": \"dense\",   # try \"sparse\" too\n",
    "        \"name\": \"physnet_memmap\",\n",
    "        \"ckpt_dir\": None,\n",
    "        \"seed\": 0,\n",
    "    }\n",
    "    defaults.update(overrides)\n",
    "    for k,v in defaults.items():\n",
    "        print(k,v)\n",
    "    return type(\"Args\", (), defaults)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a67761ab-0f03-4ce4-80ff-c975f8546546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_path openqdc_packed_memmap\n",
      "valid_split 0.125\n",
      "features 128\n",
      "max_degree 0\n",
      "num_iterations 5\n",
      "num_basis_functions 128\n",
      "cutoff 5.0\n",
      "num_atoms 128\n",
      "n_res 3\n",
      "batch_size 124\n",
      "num_epochs 10\n",
      "learning_rate 0.0001\n",
      "energy_weight 1000.0\n",
      "forces_weight 0.1\n",
      "bucket_size 8192\n",
      "graph_mode dense\n",
      "name physnet124dense\n",
      "ckpt_dir None\n",
      "seed 0\n"
     ]
    }
   ],
   "source": [
    "BS = 124\n",
    "MODE = \"dense\"\n",
    "args = make_dummy_args(name=f\"physnet{BS}{MODE}\", batch_size=BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f404c677-7d0e-48d7-8579-3323bd4e529d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PhysNet Training (Memmap + JAX graph)\n",
      "================================================================================\n",
      "__dict__: <attribute '__dict__' of 'Args' objects>\n",
      "__doc__: None\n",
      "__module__: __main__\n",
      "__weakref__: <attribute '__weakref__' of 'Args' objects>\n",
      "batch_size: 124\n",
      "bucket_size: 8192\n",
      "ckpt_dir: None\n",
      "cutoff: 5.0\n",
      "data_path: openqdc_packed_memmap\n",
      "energy_weight: 1000.0\n",
      "features: 128\n",
      "forces_weight: 0.1\n",
      "graph_mode: dense\n",
      "learning_rate: 0.0001\n",
      "max_degree: 0\n",
      "n_res: 3\n",
      "name: physnet124dense\n",
      "num_atoms: 128\n",
      "num_basis_functions: 128\n",
      "num_epochs: 10\n",
      "num_iterations: 5\n",
      "seed: 0\n",
      "valid_split: 0.125\n",
      "================================================================================\n",
      "Total molecules: 998243 | Train: 873463 | Valid: 124780\n",
      "Model initialized with 428,280 parameters\n",
      "\n",
      "Starting training...\n",
      "================================================================================\n",
      "\n",
      "Epoch 1/10\n",
      "--------------------------------------------------------------------------------\n",
      "  Batch 0: Loss=121267288.000000  E_MAE=461.572144  F_MAE=23.954142\n"
     ]
    }
   ],
   "source": [
    "main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61852fcb-efac-455c-bdb1-e60ea0961f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f286d547-3bf5-4128-b503-f0fd93d955de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bad12b0-90c3-444b-8e62-f541403dfdec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda",
   "language": "python",
   "name": "conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
