# CO2 Training Guide - Complete Workflow

This guide walks you through the complete workflow from raw data to trained models.

## Overview

This directory provides a complete pipeline for training neural network models on CO2 molecular data:

1. **Data Preparation** - Convert units and create splits
2. **PhysNet Training** - Train energy/force/dipole prediction model
3. **DCMNet Training** - Train ESP prediction model

## Directory Structure

```
examples/co2/
├── fix_and_split_cli.py          # Data preparation CLI
├── example_load_preclassified.py # Data loading example
├── CLI_USAGE.md                  # Detailed CLI documentation
├── CLI_QUICKREF.txt              # Quick reference card
├── TRAINING_GUIDE.md             # This file
│
├── preclassified_data/           # Generated by fix_and_split_cli.py
│   ├── energies_forces_dipoles_train.npz
│   ├── energies_forces_dipoles_valid.npz
│   ├── energies_forces_dipoles_test.npz
│   ├── grids_esp_train.npz
│   ├── grids_esp_valid.npz
│   ├── grids_esp_test.npz
│   ├── split_indices.npz
│   └── README.md
│
├── physnet_train/                # PhysNet training
│   ├── trainer.py
│   ├── train_default.sh
│   └── README.md
│
└── dcmnet_train/                 # DCMNet training
    ├── trainer.py
    ├── train_default.sh
    └── README.md
```

## Complete Workflow

### Step 1: Prepare Data

Convert raw NPZ files to ASE-standard units and create splits:

```bash
python fix_and_split_cli.py \
    --efd /path/to/energies_forces_dipoles.npz \
    --grid /path/to/grids_esp.npz \
    --output-dir ./preclassified_data
```

**What this does:**
- ✓ Converts energies from Hartree to eV
- ✓ Converts forces from Hartree/Bohr to eV/Angstrom
- ✓ Fixes ESP grid coordinates (indices → physical Angstroms)
- ✓ Creates 8:1:1 train/valid/test splits
- ✓ Validates all conversions
- ✓ Generates documentation

**Output:** 
- Training, validation, and test NPZ files in `preclassified_data/`
- All data in ASE-standard units, ready for training

### Step 2: Verify Data

Check that data was prepared correctly:

```bash
python example_load_preclassified.py
```

**What this does:**
- Loads training data
- Displays dataset statistics
- Shows sample structures
- Validates units

### Step 3a: Train PhysNet (Energy, Forces, Dipoles)

Train a model to predict molecular properties:

```bash
cd physnet_train
./train_default.sh

# Or with custom settings:
python trainer.py \
    --train ../preclassified_data/energies_forces_dipoles_train.npz \
    --valid ../preclassified_data/energies_forces_dipoles_valid.npz \
    --features 64 \
    --epochs 500 \
    --batch-size 16
```

**What this trains:**
- **Energies** [eV]
- **Forces** [eV/Angstrom]  
- **Dipoles** [Debye]

**Training time:**
- Small model (features=32): ~10-20 min for 100 epochs
- Default model (features=64): ~30-60 min for 100 epochs
- Large model (features=128): ~2-4 hours for 100 epochs

**Hardware:**
- CPU: Possible but slow
- GPU: Recommended (10-50x faster)

### Step 3b: Train DCMNet (ESP Prediction)

Train a model to predict electrostatic potentials:

```bash
cd dcmnet_train
./train_default.sh

# Or with custom settings:
python trainer.py \
    --train-efd ../preclassified_data/energies_forces_dipoles_train.npz \
    --train-grid ../preclassified_data/grids_esp_train.npz \
    --valid-efd ../preclassified_data/energies_forces_dipoles_valid.npz \
    --valid-grid ../preclassified_data/grids_esp_valid.npz \
    --n-dcm 4 \
    --epochs 500
```

**What this trains:**
- **ESP** [Hartree/e] on Van der Waals surfaces
- Uses distributed multipoles (monopoles + dipoles)

**Training time:**
- n_dcm=2: ~15-30 min for 100 epochs
- n_dcm=3: ~20-40 min for 100 epochs
- n_dcm=5: ~30-60 min for 100 epochs

## Quick Start Commands

### Minimal Example (< 5 minutes)

```bash
# 1. Prepare data
python fix_and_split_cli.py \
    --efd /testdata/energies_forces_dipoles.npz \
    --grid /testdata/grids_esp.npz \
    --output-dir ./preclassified_data

# 2. Train PhysNet (fast)
cd physnet_train
python trainer.py \
    --train ../preclassified_data/energies_forces_dipoles_train.npz \
    --valid ../preclassified_data/energies_forces_dipoles_valid.npz \
    --features 32 --epochs 10 --batch-size 64

# 3. Train DCMNet (fast)
cd ../dcmnet_train
python trainer.py \
    --train-efd ../preclassified_data/energies_forces_dipoles_train.npz \
    --train-grid ../preclassified_data/grids_esp_train.npz \
    --valid-efd ../preclassified_data/energies_forces_dipoles_valid.npz \
    --valid-grid ../preclassified_data/grids_esp_valid.npz \
    --n-dcm 2 --epochs 10 --batch-size 64
```

### Production Training

```bash
# 1. Prepare data (same as above)
python fix_and_split_cli.py --efd ... --grid ... --output-dir ./preclassified_data

# 2. Train PhysNet (production settings)
cd physnet_train
python trainer.py \
    --train ../preclassified_data/energies_forces_dipoles_train.npz \
    --valid ../preclassified_data/energies_forces_dipoles_valid.npz \
    --features 128 \
    --num-iterations 5 \
    --epochs 1000 \
    --batch-size 16 \
    --name co2_physnet_production

# 3. Train DCMNet (production settings)
cd ../dcmnet_train
python trainer.py \
    --train-efd ../preclassified_data/energies_forces_dipoles_train.npz \
    --train-grid ../preclassified_data/grids_esp_train.npz \
    --valid-efd ../preclassified_data/energies_forces_dipoles_valid.npz \
    --valid-grid ../preclassified_data/grids_esp_valid.npz \
    --features 64 \
    --n-dcm 5 \
    --epochs 1000 \
    --batch-size 16 \
    --name co2_dcmnet_production
```

## Model Outputs

### PhysNet Output

```
checkpoints/co2_physnet/
├── best_params.pkl              # Best model (lowest validation loss)
├── checkpoint_epoch_*.pkl       # Periodic checkpoints
└── training_history.json        # Loss curves and metrics

runs/co2_physnet/                # TensorBoard logs
└── events.out.tfevents.*
```

**Using the trained model:**

```python
import pickle
from mmml.physnetjax.physnetjax.models.model import EF

with open('checkpoints/co2_physnet/best_params.pkl', 'rb') as f:
    params = pickle.load(f)

model = EF(features=64, ...)
E, F, D = model.apply(params, R, Z, ...)
```

### DCMNet Output

```
checkpoints/
├── co2_dcmnet_epoch_5.pkl
├── co2_dcmnet_epoch_10.pkl
├── ...
└── co2_dcmnet_final.pkl         # Final trained model
```

**Using the trained model:**

```python
import pickle
from mmml.dcmnet.dcmnet.modules import MessagePassingModel
from mmml.dcmnet.dcmnet.electrostatics import calc_esp

with open('checkpoints/co2_dcmnet_final.pkl', 'rb') as f:
    params = pickle.load(f)

model = MessagePassingModel(n_dcm=3, ...)
mono, dipo = model.apply(params, Z, R, dst_idx, src_idx)
esp = calc_esp(mono, dipo, R, vdw_surface)
```

## Monitoring Training

### TensorBoard (PhysNet only)

```bash
# Start TensorBoard
tensorboard --logdir runs/

# Open browser to http://localhost:6006
```

View:
- Training/validation loss curves
- Energy/force/dipole MAE
- Learning rate schedule
- Gradient norms

### Console Output

Both trainers print progress to console:

```
Epoch 10/100:
  Train Loss: 0.0523
  Valid Loss: 0.0487
  Energy MAE: 0.012 eV
  Forces MAE: 0.156 eV/Å
  Time: 45.2s
```

## Hyperparameter Tuning

### PhysNet Key Hyperparameters

| Parameter | Small | Default | Large | Effect |
|-----------|-------|---------|-------|--------|
| `--features` | 32 | 64 | 128 | Model capacity |
| `--num-iterations` | 2 | 3 | 5 | Message passing depth |
| `--cutoff` | 5.0 | 6.0 | 8.0 | Interaction range |
| `--batch-size` | 64 | 32 | 16 | Memory/speed tradeoff |
| `--learning-rate` | 0.0005 | 0.001 | 0.002 | Convergence speed |

### DCMNet Key Hyperparameters

| Parameter | Small | Default | Large | Effect |
|-----------|-------|---------|-------|--------|
| `--features` | 16 | 32 | 64 | Model capacity |
| `--n-dcm` | 2 | 3 | 5 | Multipole accuracy |
| `--cutoff` | 8.0 | 10.0 | 12.0 | ESP accuracy |
| `--esp-weight` | 5000 | 10000 | 50000 | ESP emphasis |
| `--batch-size` | 64 | 32 | 16 | Memory/speed tradeoff |

## Troubleshooting

### Out of GPU Memory

```bash
# Reduce batch size
python trainer.py ... --batch-size 8

# Use smaller model
python trainer.py ... --features 32 --num-iterations 2
```

### Training Too Slow

```bash
# Increase batch size (if memory allows)
python trainer.py ... --batch-size 64

# Use smaller model
python trainer.py ... --features 32
```

### Poor Convergence

```bash
# Adjust learning rate
python trainer.py ... --learning-rate 0.0005

# Try different optimizer
python trainer.py ... --optimizer adam

# Adjust loss weights (PhysNet)
python trainer.py ... --forces-weight 100.0
```

### Import Errors

```bash
# Install missing dependencies
pip install lovely-jax

# Or use conda environment
conda activate mmml-gpu
```

## Testing Trained Models

After training, test on held-out test set:

```python
import numpy as np
import pickle

# Load test data
test_data = np.load('preclassified_data/energies_forces_dipoles_test.npz')

# Load model
with open('checkpoints/co2_physnet/best_params.pkl', 'rb') as f:
    params = pickle.load(f)

# Predict and evaluate
# (see model-specific READMEs for details)
```

## Best Practices

1. **Always validate data** - Run `example_load_preclassified.py` first
2. **Start small** - Test with reduced epochs/features before full training
3. **Monitor convergence** - Use TensorBoard or watch console output
4. **Save checkpoints** - Training can be interrupted and resumed
5. **Test thoroughly** - Evaluate on test set after training
6. **Document settings** - Keep track of hyperparameters used

## Performance Expectations

### CO2 Dataset (10,000 samples)

**PhysNet:**
- Energy MAE: < 0.01 eV (good), < 0.005 eV (excellent)
- Force MAE: < 0.5 eV/Å (good), < 0.2 eV/Å (excellent)
- Dipole MAE: < 0.1 Debye (good), < 0.05 Debye (excellent)

**DCMNet:**
- ESP MAE: < 0.01 Ha/e (good), < 0.005 Ha/e (excellent)

## Additional Resources

- `CLI_USAGE.md` - Data preparation guide
- `physnet_train/README.md` - PhysNet details
- `dcmnet_train/README.md` - DCMNet details
- `CLI_QUICKREF.txt` - Command quick reference

## Support

For questions or issues:
- Check the individual README files
- Review TensorBoard logs
- Open an issue on the MMML repository
- Consult the main MMML documentation

## Summary

This complete pipeline provides:
- ✅ Automated data preparation with validation
- ✅ Production-ready training scripts
- ✅ Comprehensive documentation
- ✅ Quick-start examples
- ✅ Flexible hyperparameter tuning
- ✅ Professional checkpointing and logging

You can go from raw data to trained models in **< 1 hour**!

