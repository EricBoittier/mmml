#!/usr/bin/env python3
"""
DCMNet Training Script for CO2 ESP Data

This script trains a DCMNet model on the preclassified CO2 ESP data
generated by fix_and_split_cli.py. It predicts electrostatic potentials
using distributed multipoles.

Usage:
    python trainer.py --train-efd ../preclassified_data/energies_forces_dipoles_train.npz \
                      --train-grid ../preclassified_data/grids_esp_train.npz \
                      --valid-efd ../preclassified_data/energies_forces_dipoles_valid.npz \
                      --valid-grid ../preclassified_data/grids_esp_valid.npz
    
    # Or with custom settings:
    python trainer.py --train-efd ../preclassified_data/energies_forces_dipoles_train.npz \
                      --train-grid ../preclassified_data/grids_esp_train.npz \
                      --valid-efd ../preclassified_data/energies_forces_dipoles_valid.npz \
                      --valid-grid ../preclassified_data/grids_esp_valid.npz \
                      --batch-size 16 \
                      --epochs 500 \
                      --n-dcm 3
"""

import sys
import argparse
from pathlib import Path
import numpy as np
import pickle
from typing import Dict, Tuple, Optional, Any, Mapping

# Add mmml to path
repo_root = Path(__file__).parent / "../../.."
sys.path.insert(0, str(repo_root.resolve()))

import jax
import jax.numpy as jnp
from mmml.dcmnet.dcmnet.modules import MessagePassingModel
from mmml.dcmnet.dcmnet.training import train_model
from mmml.dcmnet.dcmnet.data import prepare_datasets

CLI_PATH_FIELDS = {
    "train_efd",
    "train_grid",
    "valid_efd",
    "valid_grid",
    "output_dir",
    "restart",
}


def load_co2_data(efd_file: Path, grid_file: Path) -> Dict:
    """
    Load CO2 data from NPZ files and prepare for DCMnet training.
    
    Parameters
    ----------
    efd_file : Path
        Path to energies_forces_dipoles NPZ file
    grid_file : Path
        Path to grids_esp NPZ file
        
    Returns
    -------
    Dict
        Combined data dictionary with all required keys
    """
    efd_data = np.load(efd_file)
    grid_data = np.load(grid_file)
    
    # Combine into single dictionary
    data = {
        'R': grid_data['R'],  # Atomic coordinates [Angstrom]
        'Z': grid_data['Z'],  # Atomic numbers
        'N': grid_data['N'],  # Number of atoms
        'esp': grid_data['esp'],  # ESP values [Hartree/e]
        'vdw_surface': grid_data['vdw_surface'],  # Grid coords [Angstrom]
        'Dxyz': grid_data.get('Dxyz', efd_data['Dxyz']),  # Dipoles [Debye]
    }
    
    # Also include energies if available for reference
    if 'E' in efd_data:
        data['E'] = efd_data['E']
    
    return data


def run_single_training(args: argparse.Namespace) -> Path:
    """
    Execute a single DCMNet training run using the provided configuration.

    Parameters
    ----------
    args : argparse.Namespace
        Namespace containing all arguments required for training.

    Returns
    -------
    Path
        Path to the final checkpoint written to disk.
    """

    print("="*70)
    print("DCMNet Training - CO2 ESP Data")
    print("="*70)

    # Validate input files
    for fname, fpath in [
        ('Train EFD', args.train_efd),
        ('Train Grid', args.train_grid),
        ('Valid EFD', args.valid_efd),
        ('Valid Grid', args.valid_grid)
    ]:
        if not fpath.exists():
            print(f"âŒ Error: {fname} file not found: {fpath}")
            raise FileNotFoundError(f"{fname} file not found: {fpath}")

    print(f"\nðŸ“ Data Files:")
    print(f"  Train EFD:  {args.train_efd}")
    print(f"  Train Grid: {args.train_grid}")
    print(f"  Valid EFD:  {args.valid_efd}")
    print(f"  Valid Grid: {args.valid_grid}")

    # Setup output directory
    args.output_dir.mkdir(exist_ok=True, parents=True)
    print(f"  Output: {args.output_dir / args.name}")

    # Load data
    print(f"\n{'#'*70}")
    print("# Loading Data")
    print(f"{'#'*70}")

    if args.verbose:
        print(f"\nLoading training data...")
    train_data_raw = load_co2_data(args.train_efd, args.train_grid)

    if args.verbose:
        print(f"Loading validation data...")
    valid_data_raw = load_co2_data(args.valid_efd, args.valid_grid)

    print(f"\nâœ… Data loaded:")
    print(f"  Training samples: {len(train_data_raw['R'])}")
    print(f"  Validation samples: {len(valid_data_raw['R'])}")
    print(f"  Data keys: {list(train_data_raw.keys())}")

    # Prepare datasets (convert to DCMnet format with edge lists, etc.)
    print(f"\nPreparing datasets (computing edge lists, etc.)...")
    train_data, valid_data = prepare_datasets(
        train_data_raw,
        valid_data_raw,
        cutoff=args.cutoff,
        batch_size=args.batch_size,
    )

    print(f"âœ… Datasets prepared")
    print(f"  Training batches: {len(train_data)}")
    print(f"  Validation batches: {len(valid_data)}")

    # Build model
    print(f"\n{'#'*70}")
    print("# Building Model")
    print(f"{'#'*70}")

    print(f"\nModel hyperparameters:")
    print(f"  Features: {args.features}")
    print(f"  Max degree: {args.max_degree}")
    print(f"  Message passing iterations: {args.num_iterations}")
    print(f"  Basis functions: {args.num_basis_functions}")
    print(f"  Cutoff: {args.cutoff} Ã…")
    print(f"  Distributed multipoles per atom: {args.n_dcm}")
    print(f"  Include pseudotensors: {args.include_pseudotensors}")

    model = MessagePassingModel(
        features=args.features,
        max_degree=args.max_degree,
        num_iterations=args.num_iterations,
        num_basis_functions=args.num_basis_functions,
        cutoff=args.cutoff,
        n_dcm=args.n_dcm,
        include_pseudotensors=args.include_pseudotensors,
    )

    print(f"\nâœ… Model created: DCMNet (n_dcm={args.n_dcm})")

    # Training setup
    print(f"\n{'#'*70}")
    print("# Training Setup")
    print(f"{'#'*70}")

    print(f"\nTraining hyperparameters:")
    print(f"  Batch size: {args.batch_size}")
    print(f"  Epochs: {args.epochs}")
    print(f"  Learning rate: {args.learning_rate}")
    print(f"  ESP weight: {args.esp_weight}")
    print(f"  Random seed: {args.seed}")

    # Load restart parameters if provided
    restart_params = None
    if args.restart:
        print(f"\nðŸ“‚ Loading restart checkpoint: {args.restart}")
        with open(args.restart, 'rb') as f:
            restart_params = pickle.load(f)
        print(f"âœ… Checkpoint loaded")

    # Initialize JAX random key
    key = jax.random.PRNGKey(args.seed)

    # Start training
    print(f"\n{'='*70}")
    print("STARTING TRAINING")
    print(f"{'='*70}\n")

    final_path: Optional[Path] = None
    try:
        final_params = train_model(
            key=key,
            model=model,
            train_data=train_data,
            valid_data=valid_data,
            num_epochs=args.epochs,
            learning_rate=args.learning_rate,
            batch_size=args.batch_size,
            esp_w=args.esp_weight,
            restart_params=restart_params,
            name=args.name,
            output_dir=args.output_dir,
            print_freq=args.print_freq,
            save_freq=args.save_freq,
        )

        # Save final model
        final_path = args.output_dir / f"{args.name}_final.pkl"
        with open(final_path, 'wb') as f:
            pickle.dump(final_params, f)

        print(f"\n{'='*70}")
        print("âœ… TRAINING COMPLETE!")
        print(f"{'='*70}")
        print(f"\nFinal parameters saved to: {final_path}")
        print(f"\nTo use the trained model:")
        print(f"  from mmml.dcmnet.dcmnet.modules import MessagePassingModel")
        print(f"  import pickle")
        print(f"  ")
        print(f"  # Load parameters")
        print(f"  with open('{final_path}', 'rb') as f:")
        print(f"      params = pickle.load(f)")
        print(f"  ")
        print(f"  # Create model and predict")
        print(f"  model = MessagePassingModel(...)")
        print(f"  mono, dipo = model.apply(params, Z, R, dst_idx, src_idx)")
        print(f"  ")
        print(f"  # Calculate ESP")
        print(f"  from mmml.dcmnet.dcmnet.electrostatics import calc_esp")
        print(f"  esp_pred = calc_esp(mono, dipo, R, vdw_surface)")

    except KeyboardInterrupt:
        print(f"\n\nâš ï¸  Training interrupted by user")
        print(f"Checkpoints saved to: {args.output_dir}")
        raise
    except Exception as e:
        print(f"\n\nâŒ Training failed with error:")
        print(f"  {e}")
        import traceback
        traceback.print_exc()
        raise

    if final_path is None:
        raise RuntimeError("Training finished without producing a final checkpoint path.")

    return final_path


def build_argument_parser() -> argparse.ArgumentParser:
    """
    Create the argument parser for single-run training.

    Returns
    -------
    argparse.ArgumentParser
        Configured parser with all CLI arguments.
    """
    parser = argparse.ArgumentParser(
        description="Train DCMNet on CO2 ESP data",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )

    # Data arguments
    parser.add_argument(
        '--train-efd',
        type=Path,
        required=True,
        help='Path to training EFD NPZ file'
    )
    parser.add_argument(
        '--train-grid',
        type=Path,
        required=True,
        help='Path to training grid NPZ file'
    )
    parser.add_argument(
        '--valid-efd',
        type=Path,
        required=True,
        help='Path to validation EFD NPZ file'
    )
    parser.add_argument(
        '--valid-grid',
        type=Path,
        required=True,
        help='Path to validation grid NPZ file'
    )
    
    # Model hyperparameters
    parser.add_argument('--features', '--n-feat', type=int, default=32,
                       help='Number of features per atom')
    parser.add_argument('--max-degree', type=int, default=2,
                       help='Maximum spherical harmonic degree')
    parser.add_argument('--num-iterations', '--n-mp', type=int, default=2,
                       help='Number of message passing iterations')
    parser.add_argument('--num-basis-functions', '--n-basis', type=int, default=32,
                       help='Number of radial basis functions')
    parser.add_argument('--cutoff', type=float, default=10.0,
                       help='Cutoff distance in Angstroms')
    parser.add_argument('--n-dcm', type=int, default=3,
                       help='Number of distributed multipoles per atom')
    parser.add_argument('--include-pseudotensors', action='store_true',
                       help='Include pseudotensor features')
    
    # Training hyperparameters
    parser.add_argument('--batch-size', type=int, default=32,
                       help='Batch size')
    parser.add_argument('--epochs', '--num-epochs', type=int, default=100,
                       help='Number of epochs')
    parser.add_argument('--learning-rate', '--lr', type=float, default=0.001,
                       help='Learning rate')
    parser.add_argument('--esp-weight', '--esp-w', type=float, default=10000.0,
                       help='Weight for ESP loss')
    parser.add_argument('--seed', type=int, default=42,
                       help='Random seed')
    
    # Training options
    parser.add_argument('--restart', type=str, default=None,
                       help='Path to restart checkpoint')
    parser.add_argument('--name', type=str, default='co2_dcmnet',
                       help='Experiment name')
    parser.add_argument('--output-dir', type=Path, default=Path('./checkpoints'),
                       help='Output directory for checkpoints')
    parser.add_argument('--print-freq', type=int, default=10,
                       help='Print frequency (batches)')
    parser.add_argument('--save-freq', type=int, default=5,
                       help='Checkpoint save frequency (epochs)')
    parser.add_argument('--verbose', action='store_true', default=True,
                       help='Verbose output')
    return parser


class NotebookArgsHelper:
    """
    Convenience wrapper for assembling CLI-equivalent arguments inside notebooks.

    Examples
    --------
    >>> helper = NotebookArgsHelper(
    ...     train_efd="/abs/path/train.npz",
    ...     train_grid="/abs/path/train_grid.npz",
    ...     valid_efd="/abs/path/valid.npz",
    ...     valid_grid="/abs/path/valid_grid.npz",
    ...     output_dir="/abs/path/checkpoints",
    ...     name="notebook_experiment",
    ... )
    >>> args = helper.to_namespace()
    >>> run_single_training(args)
    """

    def __init__(self, **overrides: Any) -> None:
        parser = build_argument_parser()
        defaults = vars(parser.parse_args([]))
        self._data: Dict[str, Any] = {**defaults, **overrides}
        self._normalize_paths()

    def _normalize_paths(self) -> None:
        for key in CLI_PATH_FIELDS:
            value = self._data.get(key)
            if value is None or isinstance(value, Path):
                continue
            self._data[key] = Path(str(value)).expanduser()

    def update(self, mapping: Mapping[str, Any]) -> None:
        """
        Update stored arguments with values from *mapping*.
        """
        for key, value in mapping.items():
            if key not in self._data:
                raise KeyError(f"Unknown argument '{key}'")
            self._data[key] = value
        self._normalize_paths()

    def set(self, key: str, value: Any) -> None:
        """
        Update a single key/value pair.
        """
        self.update({key: value})

    def to_namespace(self) -> argparse.Namespace:
        """
        Return an argparse.Namespace compatible with `run_single_training`.
        """
        return argparse.Namespace(**self._data)

    def as_dict(self) -> Dict[str, Any]:
        """
        Retrieve a shallow copy of the underlying argument dictionary.
        """
        return dict(self._data)


def main():
    parser = build_argument_parser()
    args = parser.parse_args()

    try:
        run_single_training(args)
    except KeyboardInterrupt:
        sys.exit(0)
    except Exception:
        sys.exit(1)


if __name__ == "__main__":
    main()

