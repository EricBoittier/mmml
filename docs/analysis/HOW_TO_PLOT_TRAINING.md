# How to Plot Training Runs

## üìä Complete Guide to Visualizing Training

### Option 1: Extract Metrics from Orbax Checkpoints (Recommended)

**Use this when:** You have Orbax checkpoint directories with `epoch-*/` subdirectories

```bash
# Plot glycol training with log scale (recommended!)
python -m mmml.cli.extract_checkpoint_metrics \
    examples/glycol/checkpoints/glycol_production/glycol_production-*/  \
    --output glycol_training.png \
    --log-loss
```

**Features:**
- ‚úÖ Extracts actual loss values from checkpoints
- ‚úÖ **Log scale for loss** (shows improvement better)
- ‚úÖ Energy, Forces, Dipole MAE plots
- ‚úÖ Learning rate progression
- ‚úÖ Loss improvement over time
- ‚úÖ Identifies best epoch
- ‚úÖ Convergence analysis

**Output:** Comprehensive 7-panel training plot with statistics

---

### Option 2: Plot from JSON History (If Available)

**Use this when:** Training saved `history.json`

```bash
# For train_joint.py runs (saves history.json)
python -m mmml.cli.plot_training \
    checkpoints/co2_esp_model/history.json \
    --output training_curves.png
```

---

### Option 3: Analyze Checkpoint Frequency

**Use this when:** You just want to see which epochs were saved

```bash
python -m mmml.cli.plot_checkpoint_history \
    checkpoints/run/run-uuid/ \
    --output checkpoint_analysis.png
```

**Shows:**
- Checkpoint save frequency
- Checkpoint sizes over time
- Save interval distribution
- Training progress timeline

---

## üéØ Glycol Production Training Results

### Training Statistics

**From:** `glycol_production-f535e3cf-4041-4a56-bab1-e3e81cd35ab3`

```
Epochs analyzed: 53 checkpoints (epochs 1-99)
Training time: ~100 epochs

Loss:
  Initial:    14,099 (training), 22,793 (validation)
  Final:      8.64 (training), 1.11 (validation)
  Best:       7.29 @ epoch 89 (training)
             1.11 @ epoch 99 (validation)
  Improvement: 99.9%! üéâ

Validation MAE:
  Energy:  0.211 eV (best: 0.197 @ epoch 72)
  Forces:  0.047 eV/√Ö (best: 0.047 @ epoch 89)
  Dipole:  0.000 e¬∑√Ö (perfect!)

Convergence:
  Status: Nearly converged (3.86% std in last 10 epochs)
  Recommendation: Could stop here or run a few more epochs
```

### Key Findings

- ‚úÖ **Excellent convergence:** 99.9% improvement!
- ‚úÖ **Low final MAE:** 0.21 eV energy, 0.047 eV/√Ö forces
- ‚úÖ **Perfect dipoles:** 0.000 e¬∑√Ö (likely not trained with dipole loss)
- ‚úÖ **Nearly converged:** 3.86% variance in last 10 epochs
- ‚úÖ **Best checkpoint:** Epoch 99

---

## üìà Understanding the Plots

### Plot 1: Training & Validation Loss (Log Scale)
- **Blue circles:** Training loss
- **Orange squares:** Validation loss  
- **Gold star:** Best validation checkpoint
- **Shows:** Overall training progress

**Why log scale?**
- Early training: loss drops from 20,000 ‚Üí 1,000 (huge change)
- Late training: loss drops from 10 ‚Üí 1 (small change)
- Log scale shows both clearly!

### Plot 2: Best Loss Progression
- **Green line:** Best validation loss so far
- **Shows:** Monotonic improvement (good sign!)
- **Flat regions:** Plateau periods (normal)

### Plot 3: Energy MAE
- **Orange:** Validation energy MAE
- **Shows:** How well model predicts energies
- **Best:** 0.197 eV @ epoch 72

### Plot 4: Forces MAE  
- **Purple:** Validation forces MAE
- **Shows:** How well model predicts forces
- **Best:** 0.047 eV/√Ö @ epoch 89

### Plot 5: Dipole MAE
- **Sky blue:** Validation dipole MAE
- **Shows:** Dipole prediction accuracy
- **Note:** 0.000 indicates dipoles not in loss function

### Plot 6: Learning Rate
- **Yellow:** Learning rate over time
- **Shows:** Whether LR schedule is used
- **Flat:** Constant LR (1e-3)

### Plot 7: Loss Improvement
- **Green area:** % improvement from start
- **Shows:** Relative progress
- **Final:** 100% improvement (near-perfect!)

---

## üîç Interpreting Your Results

### Good Signs ‚úÖ
- Validation loss decreasing
- Training/validation losses close (no overfitting)
- MAE metrics improving
- Smooth curves (stable training)
- Best epoch near the end

### Warning Signs ‚ö†Ô∏è
- Validation loss increasing (overfitting)
- Large train/valid gap (overfitting)
- Erratic curves (unstable training)
- Best epoch early then plateau (stopped improving)

### Your Glycol Training: **EXCELLENT** ‚úÖ
- Smooth decrease
- 99.9% improvement
- Nearly converged
- No overfitting signs
- Best epoch at end

---

## üí° Quick Commands Reference

### Extract and plot metrics (with log loss)
```bash
python -m mmml.cli.extract_checkpoint_metrics \
    checkpoints/run/run-uuid/ \
    -o training.png \
    --log-loss
```

### Analyze checkpoint frequency
```bash
python -m mmml.cli.extract_checkpoint_metrics \
    checkpoints/run/run-uuid/ \
    -o analysis.png
```

### Evaluate best checkpoint
```bash
# Use the epoch from the analysis output
python -m mmml.cli.evaluate_model \
    checkpoints/run/run-uuid/epoch-99 \
    --test-data splits/data_test.npz
```

### Inspect model parameters
```bash
python -m mmml.cli.inspect_checkpoint \
    checkpoints/run/run-uuid/epoch-99
```

---

## üìä Generated Plots for Glycol

1. **glycol_training_metrics.png** - Full training analysis (7 panels)
2. **glycol_training_analysis.png** - Checkpoint frequency analysis

Both at 300 DPI, publication quality!

---

## üéì Best Practices

1. **Always use `--log-loss`** - Shows full training dynamic
2. **Check convergence** - Last 10 epochs should be stable
3. **Evaluate best checkpoint** - Not always the last epoch!
4. **Monitor overfitting** - Train/valid gap should be small
5. **Save plots** - Document your training for papers

---

## üìÅ File Locations

```
analysis/
‚îú‚îÄ‚îÄ glycol_training_metrics.png    - Full metrics plot
‚îú‚îÄ‚îÄ glycol_training_analysis.png   - Checkpoint analysis
‚îî‚îÄ‚îÄ training_summary.txt            - Text summary
```

---

## ‚úÖ Summary

Your glycol production training looks **excellent**:
- 99.9% loss improvement
- 0.21 eV energy MAE
- 0.047 eV/√Ö forces MAE
- Nearly converged
- Ready for evaluation on test set!

Use epoch-99 for production predictions.

