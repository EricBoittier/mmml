{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05f1132d-92d7-4a66-96dc-07d3e4e2a3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state:\n",
      ". . .\n",
      ". . .\n",
      ". . .\n",
      "To move: X\n",
      "\n",
      "MCTS chooses action (0..8): 4\n",
      "\n",
      "After X plays:\n",
      ". . .\n",
      ". X .\n",
      ". . .\n",
      "To move: O\n",
      "\n",
      "MCTS chooses O action: 5\n",
      "\n",
      "After O plays:\n",
      ". . .\n",
      ". X O\n",
      ". . .\n",
      "To move: X\n",
      "\n",
      "MCTS chooses action (0..8): 2\n",
      "\n",
      "After X plays:\n",
      ". . X\n",
      ". X O\n",
      ". . .\n",
      "To move: O\n",
      "\n",
      "MCTS chooses O action: 6\n",
      "\n",
      "After O plays:\n",
      ". . X\n",
      ". X O\n",
      "O . .\n",
      "To move: X\n",
      "\n",
      "MCTS chooses action (0..8): 0\n",
      "\n",
      "After X plays:\n",
      "X . X\n",
      ". X O\n",
      "O . .\n",
      "To move: O\n",
      "\n",
      "MCTS chooses O action: 8\n",
      "\n",
      "After O plays:\n",
      "X . X\n",
      ". X O\n",
      "O . O\n",
      "To move: X\n",
      "\n",
      "MCTS chooses action (0..8): 1\n",
      "\n",
      "After X plays:\n",
      "X X X\n",
      ". X O\n",
      "O . O\n",
      "To move: O\n",
      "\n",
      "MCTS chooses O action: 7\n",
      "\n",
      "After O plays:\n",
      "X X X\n",
      ". X O\n",
      "O O O\n",
      "To move: X\n",
      "\n",
      "MCTS chooses action (0..8): 3\n",
      "\n",
      "After X plays:\n",
      "X X X\n",
      "X X O\n",
      "O O O\n",
      "To move: O\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "a cannot be empty unless no samples are taken",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 222\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28mprint\u001b[39m(env)\n\u001b[1;32m    221\u001b[0m \u001b[38;5;66;03m# Now choose a reply for O (also with MCTS)\u001b[39;00m\n\u001b[0;32m--> 222\u001b[0m action_O \u001b[38;5;241m=\u001b[39m \u001b[43mmcts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_simulations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMCTS chooses O action: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction_O\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    224\u001b[0m env \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action_O)\n",
      "Cell \u001b[0;32mIn[2], line 119\u001b[0m, in \u001b[0;36mMCTS.search\u001b[0;34m(self, root_env, n_simulations)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m best_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     acts \u001b[38;5;241m=\u001b[39m root_env\u001b[38;5;241m.\u001b[39mlegal_actions()\n\u001b[0;32m--> 119\u001b[0m     best_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrng\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43macts\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(best_action)\n",
      "File \u001b[0;32mnumpy/random/_generator.pyx:803\u001b[0m, in \u001b[0;36mnumpy.random._generator.Generator.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: a cannot be empty unless no samples are taken"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Tuple, Dict\n",
    "\n",
    "# -----------------------------\n",
    "# TicTacToe environment (3x3)\n",
    "# -----------------------------\n",
    "class TicTacToe:\n",
    "    \"\"\"\n",
    "    Board: 3x3 np.int8\n",
    "      1 -> 'X', -1 -> 'O', 0 -> empty\n",
    "    Player to move: 1 or -1\n",
    "    \"\"\"\n",
    "    def __init__(self, board: Optional[np.ndarray] = None, to_move: int = 1):\n",
    "        self.board = np.zeros((3,3), dtype=np.int8) if board is None else board.copy()\n",
    "        self.to_move = int(to_move)\n",
    "\n",
    "    def clone(self) -> \"TicTacToe\":\n",
    "        return TicTacToe(self.board, self.to_move)\n",
    "\n",
    "    def legal_actions(self) -> np.ndarray:\n",
    "        \"\"\"Actions are flat indices [0..8] for empty cells.\"\"\"\n",
    "        return np.flatnonzero(self.board.ravel() == 0)\n",
    "\n",
    "    def step(self, action: int) -> \"TicTacToe\":\n",
    "        \"\"\"Return next state after placing current player's mark at action.\"\"\"\n",
    "        r, c = divmod(int(action), 3)\n",
    "        if self.board[r, c] != 0:\n",
    "            raise ValueError(\"Illegal action\")\n",
    "        nxt = self.clone()\n",
    "        nxt.board[r, c] = self.to_move\n",
    "        nxt.to_move = -self.to_move\n",
    "        return nxt\n",
    "\n",
    "    def is_terminal(self) -> bool:\n",
    "        return self.winner() is not None or self.legal_actions().size == 0\n",
    "\n",
    "    def winner(self) -> Optional[int]:\n",
    "        \"\"\"Return 1, -1 for winner; 0 for draw; None if not terminal.\"\"\"\n",
    "        lines = []\n",
    "        lines.extend([self.board[i, :] for i in range(3)])\n",
    "        lines.extend([self.board[:, j] for j in range(3)])\n",
    "        lines.append(np.diag(self.board))\n",
    "        lines.append(np.diag(np.fliplr(self.board)))\n",
    "        for line in lines:\n",
    "            s = np.sum(line)\n",
    "            if s == 3:  return 1\n",
    "            if s == -3: return -1\n",
    "        if np.all(self.board != 0):\n",
    "            return 0\n",
    "        return None\n",
    "\n",
    "    def result_from(self, player: int) -> float:\n",
    "        \"\"\"\n",
    "        Return result from 'player' perspective at terminal:\n",
    "          1.0 win, 0.0 draw, -1.0 loss\n",
    "        \"\"\"\n",
    "        w = self.winner()\n",
    "        if w is None:\n",
    "            raise ValueError(\"Not terminal\")\n",
    "        if w == 0: return 0.0\n",
    "        return 1.0 if w == player else -1.0\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        s = {1: \"X\", -1: \"O\", 0: \".\"}\n",
    "        rows = [\" \".join(s[int(x)] for x in row) for row in self.board]\n",
    "        return \"\\n\".join(rows) + f\"\\nTo move: {'X' if self.to_move==1 else 'O'}\"\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# MCTS (UCT)\n",
    "# -----------------------------\n",
    "@dataclass\n",
    "class Node:\n",
    "    state_key: Tuple  # a hashable representation of the state\n",
    "    to_move: int      # player to move at this node\n",
    "    parent: Optional[\"Node\"] = None\n",
    "    parent_action: Optional[int] = None\n",
    "    children: Dict[int, \"Node\"] = field(default_factory=dict)\n",
    "\n",
    "    N: int = 0         # visit count\n",
    "    W: float = 0.0     # total value (from root's POV)\n",
    "    Q: float = 0.0     # mean value\n",
    "\n",
    "    untried_actions: Optional[np.ndarray] = None\n",
    "\n",
    "def state_to_key(env: TicTacToe) -> Tuple:\n",
    "    # Compact, hashable key: (tuple(board_flat), to_move)\n",
    "    return (tuple(env.board.ravel().tolist()), env.to_move)\n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, c_uct: float = np.sqrt(2.0), rollout_depth: int = 20, rng: Optional[np.random.Generator] = None):\n",
    "        self.c_uct = float(c_uct)\n",
    "        self.rollout_depth = int(rollout_depth)\n",
    "        self.rng = rng if rng is not None else np.random.default_rng()\n",
    "\n",
    "    def search(self, root_env: TicTacToe, n_simulations: int = 200) -> int:\n",
    "        \"\"\"Run MCTS from root_env and return the best action by visit count.\"\"\"\n",
    "        root = Node(state_key=state_to_key(root_env), to_move=root_env.to_move)\n",
    "        root.untried_actions = root_env.legal_actions()\n",
    "\n",
    "        for _ in range(n_simulations):\n",
    "            self._simulate(root_env, root, root_player=root_env.to_move)\n",
    "\n",
    "        # Choose action with max visit count\n",
    "        if root.untried_actions is not None and root.untried_actions.size == 9:\n",
    "            # Edge case: empty board; children might still be empty if simulations were zero.\n",
    "            pass\n",
    "\n",
    "        best_action, best_visits = None, -1\n",
    "        for a, child in root.children.items():\n",
    "            if child.N > best_visits:\n",
    "                best_visits = child.N\n",
    "                best_action = a\n",
    "\n",
    "        # If no children expanded (shouldn't happen unless n_simulations==0), pick random legal\n",
    "        if best_action is None:\n",
    "            acts = root_env.legal_actions()\n",
    "            best_action = int(self.rng.choice(acts))\n",
    "\n",
    "        return int(best_action)\n",
    "\n",
    "    # ---------- One MCTS iteration ----------\n",
    "    def _simulate(self, root_env: TicTacToe, root: Node, root_player: int):\n",
    "        # 1) Selection\n",
    "        node = root\n",
    "        env = root_env.clone()\n",
    "\n",
    "        while node.untried_actions is not None and node.untried_actions.size == 0 and node.children:\n",
    "            a = self._select_uct(node)\n",
    "            env = env.step(a)\n",
    "            node = node.children[a]\n",
    "\n",
    "        # 2) Expansion\n",
    "        if node.untried_actions is None:\n",
    "            node.untried_actions = env.legal_actions()\n",
    "        if node.untried_actions.size > 0 and not env.is_terminal():\n",
    "            # Pick one untried action uniformly\n",
    "            idx = self.rng.integers(0, node.untried_actions.size)\n",
    "            a = int(node.untried_actions[idx])\n",
    "            # Remove it from untried\n",
    "            node.untried_actions = np.delete(node.untried_actions, idx)\n",
    "            env_next = env.step(a)\n",
    "            child = Node(\n",
    "                state_key=state_to_key(env_next),\n",
    "                to_move=env_next.to_move,\n",
    "                parent=node,\n",
    "                parent_action=a\n",
    "            )\n",
    "            child.untried_actions = env_next.legal_actions()\n",
    "            node.children[a] = child\n",
    "            node = child\n",
    "            env = env_next\n",
    "\n",
    "        # 3) Simulation (rollout)\n",
    "        value = self._rollout(env, root_player)\n",
    "\n",
    "        # 4) Backpropagation\n",
    "        self._backpropagate(node, value)\n",
    "\n",
    "    def _select_uct(self, node: Node) -> int:\n",
    "        \"\"\"Select child that maximizes UCT.\"\"\"\n",
    "        assert node.children, \"Selection requires children\"\n",
    "        log_N = np.log(max(1, node.N))\n",
    "        best_a, best_score = None, -1e9\n",
    "        for a, child in node.children.items():\n",
    "            if child.N == 0:\n",
    "                uct = np.inf\n",
    "            else:\n",
    "                uct = child.Q + self.c_uct * np.sqrt(log_N / child.N)\n",
    "            if uct > best_score:\n",
    "                best_score = uct\n",
    "                best_a = a\n",
    "        return int(best_a)\n",
    "\n",
    "    def _rollout(self, env: TicTacToe, root_player: int) -> float:\n",
    "        \"\"\"Random playout until terminal or depth cutoff.\"\"\"\n",
    "        depth = 0\n",
    "        e = env.clone()\n",
    "        while not e.is_terminal() and depth < self.rollout_depth:\n",
    "            acts = e.legal_actions()\n",
    "            if acts.size == 0:  # draw\n",
    "                break\n",
    "            a = int(self.rng.choice(acts))\n",
    "            e = e.step(a)\n",
    "            depth += 1\n",
    "\n",
    "        if e.is_terminal():\n",
    "            return e.result_from(root_player)\n",
    "        else:\n",
    "            # Non-terminal cutoff -> simple heuristic (0.0 = drawish)\n",
    "            return 0.0\n",
    "\n",
    "    def _backpropagate(self, node: Node, value: float):\n",
    "        \"\"\"Accumulate value up to root (value is from root player's perspective).\"\"\"\n",
    "        cur = node\n",
    "        while cur is not None:\n",
    "            cur.N += 1\n",
    "            cur.W += value\n",
    "            cur.Q = cur.W / cur.N\n",
    "            cur = cur.parent\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Example usage\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    env = TicTacToe()  # empty board, X to move (1)\n",
    "    mcts = MCTS(c_uct=np.sqrt(2), rollout_depth=20)\n",
    "\n",
    "    print(\"Initial state:\")\n",
    "    print(env)\n",
    "    for i in range(10):\n",
    "        # Let MCTS pick a move for X\n",
    "        action = mcts.search(env, n_simulations=1000)\n",
    "        print(f\"\\nMCTS chooses action (0..8): {action}\")\n",
    "        env = env.step(action)\n",
    "        print(\"\\nAfter X plays:\")\n",
    "        print(env)\n",
    "    \n",
    "        # Now choose a reply for O (also with MCTS)\n",
    "        action_O = mcts.search(env, n_simulations=1000)\n",
    "        print(f\"\\nMCTS chooses O action: {action_O}\")\n",
    "        env = env.step(action_O)\n",
    "        print(\"\\nAfter O plays:\")\n",
    "        print(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d8434b4-f786-4d42-b4d9-d4c0c4e8d5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state:\n",
      ". . .\n",
      ". . .\n",
      ". . .\n",
      "To move: X\n",
      "\n",
      "Chosen action: 5\n",
      "\n",
      "After move:\n",
      ". . .\n",
      ". . X\n",
      ". . .\n",
      "To move: O\n",
      "\n",
      "Reply action: 4\n",
      "\n",
      "State after reply:\n",
      ". . .\n",
      ". O X\n",
      ". . .\n",
      "To move: X\n",
      "Initial state:\n",
      ". . .\n",
      ". . .\n",
      ". . .\n",
      "To move: X\n",
      "\n",
      "Chosen action: 3\n",
      "\n",
      "After move:\n",
      ". . .\n",
      "X . .\n",
      ". . .\n",
      "To move: O\n",
      "\n",
      "Reply action: 8\n",
      "\n",
      "State after reply:\n",
      ". . .\n",
      "X . .\n",
      ". . O\n",
      "To move: X\n",
      "Initial state:\n",
      ". . .\n",
      ". . .\n",
      ". . .\n",
      "To move: X\n",
      "\n",
      "Chosen action: 1\n",
      "\n",
      "After move:\n",
      ". X .\n",
      ". . .\n",
      ". . .\n",
      "To move: O\n",
      "\n",
      "Reply action: 2\n",
      "\n",
      "State after reply:\n",
      ". X O\n",
      ". . .\n",
      ". . .\n",
      "To move: X\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# Neural-guided MCTS with JAX + Flax (TicTacToe)\n",
    "# ================================================\n",
    "import numpy as np\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict, Tuple\n",
    "\n",
    "# ---------- JAX / Flax ----------\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from functools import partial\n",
    "\n",
    "# =========================================================\n",
    "# Minimal TicTacToe env (same API as before)\n",
    "# =========================================================\n",
    "class TicTacToe:\n",
    "    \"\"\"\n",
    "    Board: 3x3 np.int8\n",
    "      1 -> 'X', -1 -> 'O', 0 -> empty\n",
    "    Player to move: 1 or -1\n",
    "    \"\"\"\n",
    "    def __init__(self, board: Optional[np.ndarray] = None, to_move: int = 1):\n",
    "        self.board = np.zeros((3,3), dtype=np.int8) if board is None else board.copy()\n",
    "        self.to_move = int(to_move)\n",
    "\n",
    "    def clone(self): return TicTacToe(self.board, self.to_move)\n",
    "\n",
    "    def legal_actions(self) -> np.ndarray:\n",
    "        return np.flatnonzero(self.board.ravel() == 0)\n",
    "\n",
    "    def step(self, action: int) -> \"TicTacToe\":\n",
    "        r, c = divmod(int(action), 3)\n",
    "        if self.board[r, c] != 0: raise ValueError(\"Illegal action\")\n",
    "        nxt = self.clone()\n",
    "        nxt.board[r, c] = self.to_move\n",
    "        nxt.to_move = -self.to_move\n",
    "        return nxt\n",
    "\n",
    "    def is_terminal(self) -> bool:\n",
    "        return self.winner() is not None or self.legal_actions().size == 0\n",
    "\n",
    "    def winner(self) -> Optional[int]:\n",
    "        b = self.board\n",
    "        lines = list(b) + list(b.T) + [np.diag(b), np.diag(np.fliplr(b))]\n",
    "        for line in lines:\n",
    "            s = int(np.sum(line))\n",
    "            if s == 3:  return 1\n",
    "            if s == -3: return -1\n",
    "        if np.all(b != 0): return 0\n",
    "        return None\n",
    "\n",
    "    def result_from(self, player: int) -> float:\n",
    "        w = self.winner()\n",
    "        if w is None: raise ValueError(\"Not terminal\")\n",
    "        if w == 0: return 0.0\n",
    "        return 1.0 if w == player else -1.0\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        s = {1:\"X\",-1:\"O\",0:\".\"}\n",
    "        rows = [\" \".join(s[int(x)] for x in row) for row in self.board]\n",
    "        return \"\\n\".join(rows) + f\"\\nTo move: {'X' if self.to_move==1 else 'O'}\"\n",
    "\n",
    "# =========================================================\n",
    "# AlphaZero-style PUCT MCTS (unchanged)\n",
    "# =========================================================\n",
    "@dataclass\n",
    "class Node:\n",
    "    key: Tuple\n",
    "    to_move: int\n",
    "    parent: Optional[\"Node\"] = None\n",
    "    parent_action: Optional[int] = None\n",
    "\n",
    "    children: Dict[int, \"Node\"] = field(default_factory=dict)  # action -> child node\n",
    "    P: Dict[int, float] = field(default_factory=dict)          # action -> prior prob\n",
    "\n",
    "    N: int = 0\n",
    "    W: float = 0.0\n",
    "    Q: float = 0.0\n",
    "\n",
    "    def expanded(self) -> bool:\n",
    "        return len(self.P) > 0\n",
    "\n",
    "def state_to_key(env: TicTacToe) -> Tuple:\n",
    "    return (tuple(env.board.ravel().tolist()), env.to_move)\n",
    "\n",
    "class PUCT_MCTS:\n",
    "    \"\"\"\n",
    "    Selection: a* = argmax[ Q(s,a) + c_puct * P(s,a) * sqrt(sum_b N(s,b)) / (1 + N(s,a)) ]\n",
    "    Expansion: add new child with policy prior\n",
    "    Evaluation: value network v(s) (from current player POV)\n",
    "    Backup: update stats with conversion to root POV\n",
    "    \"\"\"\n",
    "    def __init__(self, policy_value_fn, c_puct: float = 1.5, dirichlet_alpha: float = 0.3,\n",
    "                 root_noise_frac: float = 0.25, rng: Optional[np.random.Generator] = None):\n",
    "        self.policy_value_fn = policy_value_fn\n",
    "        self.c_puct = float(c_puct)\n",
    "        self.dirichlet_alpha = float(dirichlet_alpha)\n",
    "        self.root_noise_frac = float(root_noise_frac)\n",
    "        self.rng = rng if rng is not None else np.random.default_rng()\n",
    "        self.last_root = None\n",
    "\n",
    "    def search(self, root_env: TicTacToe, n_simulations: int = 400, temperature: float = 1.0) -> int:\n",
    "        root = Node(key=state_to_key(root_env), to_move=root_env.to_move)\n",
    "        self._expand(root_env, root)\n",
    "        self._add_root_dirichlet_noise(root)\n",
    "\n",
    "        for _ in range(n_simulations):\n",
    "            self._simulate(root_env, root, root_player=root_env.to_move)\n",
    "    \n",
    "        self.last_root = root\n",
    "        return self._select_action_from_visits(root, temperature)\n",
    "\n",
    "\n",
    "    def _simulate(self, root_env: TicTacToe, root: Node, root_player: int):\n",
    "        node = root\n",
    "        env = root_env.clone()\n",
    "        path = []\n",
    "\n",
    "        while True:\n",
    "            if env.is_terminal():\n",
    "                value = env.result_from(root_player)\n",
    "                self._backup(path, leaf_value=value)\n",
    "                return\n",
    "\n",
    "            a = self._puct_select(node)\n",
    "            path.append((node, a))\n",
    "\n",
    "            if a not in node.children:\n",
    "                env = env.step(a)\n",
    "                child = Node(key=state_to_key(env), to_move=env.to_move, parent=node, parent_action=a)\n",
    "                priors, leaf_val_from_to_move = self.policy_value_fn(env)\n",
    "                child.P = priors\n",
    "                node.children[a] = child\n",
    "                self._backup(path, leaf_value=leaf_val_from_to_move, leaf_env_to_move=env.to_move, root_player=root_player)\n",
    "                return\n",
    "\n",
    "            env = env.step(a)\n",
    "            node = node.children[a]\n",
    "\n",
    "    def _puct_select(self, node: Node) -> int:\n",
    "        sum_N = max(1, sum(child.N for child in node.children.values()))\n",
    "        best_a, best_score = None, -1e18\n",
    "        for a, p in node.P.items():\n",
    "            child = node.children.get(a)\n",
    "            Nsa = 0 if child is None else child.N\n",
    "            Qsa = 0.0 if child is None else child.Q\n",
    "            u = self.c_puct * p * np.sqrt(sum_N) / (1.0 + Nsa)\n",
    "            score = Qsa + u\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_a = a\n",
    "        return int(best_a)\n",
    "\n",
    "    def _expand(self, env: TicTacToe, node: Node):\n",
    "        priors, _ = self.policy_value_fn(env)\n",
    "        node.P = priors\n",
    "\n",
    "    def _add_root_dirichlet_noise(self, root: Node):\n",
    "        if not root.P: return\n",
    "        actions = list(root.P.keys())\n",
    "        alpha = self.dirichlet_alpha\n",
    "        noise = self.rng.dirichlet([alpha] * len(actions))\n",
    "        for a, eps in zip(actions, noise):\n",
    "            root.P[a] = (1 - self.root_noise_frac) * root.P[a] + self.root_noise_frac * float(eps)\n",
    "\n",
    "    def _backup(self, path, leaf_value: float, leaf_env_to_move: Optional[int] = None, root_player: Optional[int] = None):\n",
    "        if leaf_env_to_move is None or root_player is None:\n",
    "            v_root = leaf_value\n",
    "            for node, _ in reversed(path):\n",
    "                node.N += 1; node.W += v_root; node.Q = node.W / node.N\n",
    "                v_root = -v_root\n",
    "            return\n",
    "\n",
    "        same = 1.0 if leaf_env_to_move == root_player else -1.0\n",
    "        v_root = leaf_value * same\n",
    "        for node, _ in reversed(path):\n",
    "            node.N += 1; node.W += v_root; node.Q = node.W / node.N\n",
    "            v_root = -v_root\n",
    "\n",
    "    def _select_action_from_visits(self, root: Node, temperature: float) -> int:\n",
    "        acts = np.array(list(root.P.keys()), dtype=int)\n",
    "        visits = np.array([root.children[a].N if a in root.children else 0 for a in acts], dtype=float)\n",
    "        if temperature <= 1e-8:\n",
    "            return int(acts[np.argmax(visits)])\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            pi = np.power(visits, 1.0 / temperature)\n",
    "        if np.all(pi == 0): pi = np.ones_like(pi)\n",
    "        pi = pi / np.sum(pi)\n",
    "        return int(np.random.default_rng().choice(acts, p=pi))\n",
    "\n",
    "# =========================================================\n",
    "# Flax policy/value network\n",
    "# =========================================================\n",
    "def make_features(board_np: np.ndarray, to_move: int) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Returns (3,3,3) float32:\n",
    "      ch0 = 1 where X stones\n",
    "      ch1 = 1 where O stones\n",
    "      ch2 = to_move (all +1 if X to move, all -1 if O to move)\n",
    "    \"\"\"\n",
    "    b = board_np.astype(np.int8)\n",
    "    x = (b == 1).astype(np.float32)\n",
    "    o = (b == -1).astype(np.float32)\n",
    "    tm = np.full_like(x, float(1.0 if to_move == 1 else -1.0), dtype=np.float32)\n",
    "    feat = np.stack([x, o, tm], axis=-1)  # (3,3,3)\n",
    "    return jnp.asarray(feat)\n",
    "\n",
    "class TTTNet(nn.Module):\n",
    "    \"\"\"Tiny conv net with separate policy and value heads.\"\"\"\n",
    "    @nn.compact\n",
    "    def __call__(self, x):  # x: (B,3,3,3)\n",
    "        # Trunk\n",
    "        y = nn.Conv(features=32, kernel_size=(3,3), padding='SAME')(x)\n",
    "        y = nn.relu(y)\n",
    "        y = nn.Conv(features=64, kernel_size=(3,3), padding='SAME')(y)\n",
    "        y = nn.relu(y)\n",
    "        y = y.reshape((y.shape[0], -1))\n",
    "        y = nn.Dense(64)(y); y = nn.relu(y)\n",
    "\n",
    "        # Policy head -> logits for 9 actions\n",
    "        p = nn.Dense(32)(y); p = nn.relu(p)\n",
    "        policy_logits = nn.Dense(9)(p)  # (B,9)\n",
    "\n",
    "        # Value head -> scalar in [-1,1]\n",
    "        v = nn.Dense(32)(y); v = nn.relu(v)\n",
    "        value = nn.Dense(1)(v)\n",
    "        value = nn.tanh(value)          # (B,1)\n",
    "        return policy_logits, value[:, 0]\n",
    "\n",
    "def mask_and_softmax(logits: jnp.ndarray, legal_mask: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"Softmax over legal actions only. logits, mask shape (9,), mask {0,1}.\"\"\"\n",
    "    # Set illegal to a large negative.\n",
    "    masked = jnp.where(legal_mask > 0.5, logits, -1e9)\n",
    "    # Avoid NaNs if all illegal (shouldn't happen)\n",
    "    masked = jnp.nan_to_num(masked, nan=-1e9)\n",
    "    exps = jnp.exp(masked - jnp.max(masked))\n",
    "    denom = jnp.sum(exps)\n",
    "    probs = jnp.where(denom > 0, exps / denom, jnp.ones_like(exps) / exps.shape[0])\n",
    "    return probs\n",
    "\n",
    "@partial(jax.jit, static_argnums=0)\n",
    "def model_apply_jit(model: TTTNet, params, feats: jnp.ndarray, legal_mask: jnp.ndarray):\n",
    "    \"\"\"Batched apply; feats (B,3,3,3), legal_mask (B,9).\"\"\"\n",
    "    logits, value = model.apply(params, feats)      # logits (B,9), value (B,)\n",
    "    probs = jax.vmap(mask_and_softmax)(logits, legal_mask)\n",
    "    return probs, value\n",
    "\n",
    "# Wrapper to create a policy_value_fn compatible with PUCT_MCTS\n",
    "def make_flax_policy_value_fn(model: TTTNet, params):\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    def policy_value_fn(env: TicTacToe):\n",
    "        legal = env.legal_actions()\n",
    "        # Build feature tensor (B=1)\n",
    "        feats = make_features(env.board, env.to_move)[None, ...]   # (1,3,3,3)\n",
    "        # Build legal mask (1,9)\n",
    "        mask = np.zeros((1, 9), dtype=np.float32)\n",
    "        mask[0, legal] = 1.0\n",
    "        probs_b, value_b = model_apply_jit(model, params, feats, jnp.asarray(mask))\n",
    "        probs = np.array(probs_b[0])   # (9,)\n",
    "        value = float(np.array(value_b[0]))  # from CURRENT to-move POV (as desired)\n",
    "\n",
    "        # Convert to dict over legal actions only\n",
    "        priors = {int(a): float(probs[a]) for a in legal}\n",
    "        # Normalize just in case of tiny numerical drift\n",
    "        s = sum(priors.values())\n",
    "        if s <= 0:\n",
    "            # Fallback uniform\n",
    "            u = 1.0 / max(1, len(legal))\n",
    "            priors = {int(a): u for a in legal}\n",
    "        else:\n",
    "            for a in list(priors.keys()):\n",
    "                priors[a] /= s\n",
    "        return priors, value\n",
    "\n",
    "    return policy_value_fn\n",
    "\n",
    "# =========================================================\n",
    "# Example usage\n",
    "# =========================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # Init model & params\n",
    "    model = TTTNet()\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    dummy_feats = jnp.zeros((1,3,3,3), dtype=jnp.float32)\n",
    "    params = model.init(key, dummy_feats)\n",
    "\n",
    "    # Build policy/value fn\n",
    "    pv_fn = make_flax_policy_value_fn(model, params)\n",
    "\n",
    "    # Create MCTS with neural guidance\n",
    "    mcts = PUCT_MCTS(\n",
    "        policy_value_fn=pv_fn,\n",
    "        c_puct=1.5,\n",
    "        dirichlet_alpha=0.3,\n",
    "        root_noise_frac=0.25,\n",
    "        rng=np.random.default_rng(0)\n",
    "    )\n",
    "\n",
    "    for i in range(3):\n",
    "        # Play two moves using neural-guided MCTS\n",
    "        env = TicTacToe()  # X to move\n",
    "        print(\"Initial state:\\n\", env, sep=\"\")\n",
    "        a1 = mcts.search(env, n_simulations=800, temperature=1.0)\n",
    "        print(f\"\\nChosen action: {a1}\")\n",
    "        env = env.step(a1)\n",
    "        print(\"\\nAfter move:\\n\", env, sep=\"\")\n",
    "    \n",
    "        a2 = mcts.search(env, n_simulations=800, temperature=1e-9)  # greedy by visits\n",
    "        print(f\"\\nReply action: {a2}\")\n",
    "        env = env.step(a2)\n",
    "        print(\"\\nState after reply:\\n\", env, sep=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "135cee4a-4d50-4141-9207-a9a89743048c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c49f2a13-d0ff-4819-bfbe-972d0df8b52c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a9e8cb53-ed69-4fc0-ac37-2587b8fab54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Self-play & Replay Buffer\n",
    "# ============================\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int = 100_000, rng: np.random.Generator = None):\n",
    "        self.capacity = capacity\n",
    "        self.rng = rng or np.random.default_rng()\n",
    "        self._data = []  # list of (feat: (3,3,3), pi: (9,), z: scalar)\n",
    "\n",
    "    def add_many(self, samples: List[Tuple[np.ndarray, np.ndarray, float]]):\n",
    "        if not samples: return\n",
    "        self._data.extend(samples)\n",
    "        if len(self._data) > self.capacity:\n",
    "            # Drop oldest\n",
    "            self._data = self._data[-self.capacity:]\n",
    "\n",
    "    def sample(self, batch_size: int):\n",
    "        idx = self.rng.choice(len(self._data), size=batch_size, replace=False)\n",
    "        feats = np.stack([self._data[i][0] for i in idx], axis=0).astype(np.float32)   # (B,3,3,3)\n",
    "        pis   = np.stack([self._data[i][1] for i in idx], axis=0).astype(np.float32)   # (B,9)\n",
    "        zs    = np.array([self._data[i][2] for i in idx], dtype=np.float32)            # (B,)\n",
    "        return feats, pis, zs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "\n",
    "def visits_to_pi(root_node) -> np.ndarray:\n",
    "    \"\"\"Turn root children visit counts into a probability vector over 9 actions.\"\"\"\n",
    "    pi = np.zeros(9, dtype=np.float32)\n",
    "    total = 0\n",
    "    for a, child in root_node.children.items():\n",
    "        n = child.N\n",
    "        pi[a] = n\n",
    "        total += n\n",
    "    if total > 0:\n",
    "        pi /= total\n",
    "    else:\n",
    "        # fallback uniform\n",
    "        pi[:] = 1.0 / 9.0\n",
    "    return pi\n",
    "\n",
    "def outcome_to_value_for_player(winner: int, player: int) -> float:\n",
    "    \"\"\"winner in {-1,0,1}, player in {-1,1} -> value in {-1,0,1} from player's POV.\"\"\"\n",
    "    if winner == 0: return 0.0\n",
    "    return 1.0 if winner == player else -1.0\n",
    "\n",
    "def self_play_game(mcts, model, params, temperature: float = 1.0, temp_moves: int = 6):\n",
    "    \"\"\"\n",
    "    Play one game using current network.\n",
    "    Return list of (feat, pi, z) samples.\n",
    "    - temperature anneals: high for first 'temp_moves', then low (greedy by visits).\n",
    "    \"\"\"\n",
    "    from copy import deepcopy\n",
    "    env = TicTacToe()\n",
    "    samples = []\n",
    "\n",
    "    move_idx = 0\n",
    "    states_pov: List[int] = []  # player to move for each recorded state (for value target)\n",
    "    while not env.is_terminal():\n",
    "        # Make features & run MCTS\n",
    "        feats = make_features(env.board, env.to_move)  # (3,3,3)\n",
    "        tau = temperature if move_idx < temp_moves else 1e-9\n",
    "\n",
    "        # Run MCTS with current params (wrap a flax policy/value fn)\n",
    "        pv_fn = make_flax_policy_value_fn(model, params)\n",
    "        action = mcts.search(env, n_simulations=200, temperature=tau)  # you can tune sims\n",
    "\n",
    "        # Grab pi from the root (visit counts distribution)\n",
    "        # The PUCT_MCTS doesn't return the root; quick hack: rerun expand to reconstruct priors,\n",
    "        # then get visits from the internal root held during the last search. Easiest approach:\n",
    "        # modify search() to return both action and the root node. For brevity, we just recompute here:\n",
    "        # (Instead, we store it inside mcts for this call.)\n",
    "        # ---- Small patch: add a field 'last_root' on mcts in its search() implementation ----\n",
    "        pi = visits_to_pi(mcts.last_root)\n",
    "\n",
    "        samples.append((np.array(feats, dtype=np.float32), pi.copy(), 0.0))  # z is filled later\n",
    "        states_pov.append(env.to_move)\n",
    "\n",
    "        # Play move\n",
    "        env = env.step(int(action))\n",
    "        move_idx += 1\n",
    "\n",
    "    # Game finished; set z for all samples\n",
    "    w = env.winner()  # in {-1,0,1}\n",
    "    finalized = []\n",
    "    for (feat, pi, _), pov in zip(samples, states_pov):\n",
    "        z = outcome_to_value_for_player(w, pov)\n",
    "        finalized.append((feat, pi, z))\n",
    "    return finalized\n",
    "# ============================\n",
    "# Training step (JAX/Optax)\n",
    "# ============================\n",
    "from flax.core import FrozenDict\n",
    "\n",
    "def masked_log_softmax(logits: jnp.ndarray, legal_mask: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"log softmax over legal moves only; illegal receive ~ -inf.\"\"\"\n",
    "    masked = jnp.where(legal_mask > 0.5, logits, -1e9)\n",
    "    return jax.nn.log_softmax(masked, axis=-1)\n",
    "\n",
    "def make_legal_mask_from_feats(feats: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    For TicTacToe: legal = cells where both X and O channels are 0 (empty).\n",
    "    feats: (B,3,3,3) with channels [X, O, to_move].\n",
    "    \"\"\"\n",
    "    x = feats[..., 0]\n",
    "    o = feats[..., 1]\n",
    "    empty = (x == 0) & (o == 0)\n",
    "    return empty.reshape((empty.shape[0], 9)).astype(jnp.float32)\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "@partial(jax.jit, static_argnames=['model'])\n",
    "def forward_for_train(params: FrozenDict, model: TTTNet, feats: jnp.ndarray):\n",
    "    \"\"\"Return (policy_logits, value) without masking/softmax; training handles masking.\"\"\"\n",
    "    logits, value = model.apply(params, feats)\n",
    "    return logits, value  # (B,9), (B,)\n",
    "\n",
    "def policy_value_loss(params: FrozenDict, model: TTTNet,\n",
    "                      feats: jnp.ndarray, target_pi: jnp.ndarray, target_z: jnp.ndarray,\n",
    "                      weight_policy: float = 1.0, weight_value: float = 1.0):\n",
    "    logits, value = forward_for_train(params, model, feats)  # (B,9), (B,)\n",
    "    legal_mask = jnp.asarray(make_legal_mask_from_feats(jnp.array(feats)))  # (B,9) on host -> device\n",
    "\n",
    "    logp = masked_log_softmax(logits, legal_mask)  # (B,9)\n",
    "    # cross-entropy: -sum pi * logp\n",
    "    pol_loss = -jnp.mean(jnp.sum(target_pi * logp, axis=-1))\n",
    "    val_loss = jnp.mean((value - target_z) ** 2)\n",
    "    loss = weight_policy * pol_loss + weight_value * val_loss\n",
    "    metrics = {\n",
    "        \"loss\": loss,\n",
    "        \"policy_loss\": pol_loss,\n",
    "        \"value_loss\": val_loss,\n",
    "        \"value_mae\": jnp.mean(jnp.abs(value - target_z)),\n",
    "    }\n",
    "    return loss, metrics\n",
    "\n",
    "def make_optimizer(lr: float = 1e-3, weight_decay: float = 1e-4):\n",
    "    tx = optax.adamw(learning_rate=lr, weight_decay=weight_decay)\n",
    "    return tx\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "@partial(jax.jit, static_argnames=['model', 'tx'])\n",
    "def train_step(params: FrozenDict, opt_state, model: TTTNet,\n",
    "               feats: jnp.ndarray, target_pi: jnp.ndarray, target_z: jnp.ndarray, tx):\n",
    "    (loss, metrics), grads = jax.value_and_grad(policy_value_loss, has_aux=True)(\n",
    "        params, model, feats, target_pi, target_z\n",
    "    )\n",
    "    updates, opt_state = tx.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, metrics\n",
    "\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Simple training loop\n",
    "# ============================\n",
    "def train_loop(num_iters=50, games_per_iter=16, batch_size=64, train_steps=100,\n",
    "               sims_per_move=200, lr=1e-3, wd=1e-4, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # Init model + params\n",
    "    model = TTTNet()\n",
    "    key = jax.random.PRNGKey(seed)\n",
    "    dummy_feats = jnp.zeros((1,3,3,3), dtype=jnp.float32)\n",
    "    params = model.init(key, dummy_feats)\n",
    "\n",
    "    # MCTS with neural guidance\n",
    "    pv_fn = make_flax_policy_value_fn(model, params)\n",
    "    mcts = PUCT_MCTS(\n",
    "        policy_value_fn=pv_fn,\n",
    "        c_puct=1.5,\n",
    "        dirichlet_alpha=0.3,\n",
    "        root_noise_frac=0.25,\n",
    "        rng=rng\n",
    "    )\n",
    "\n",
    "    # PATCH: store root in search() (add to PUCT_MCTS.search before returning):\n",
    "    #   self.last_root = root\n",
    "    # And use sims_per_move arg inside search\n",
    "    # (to keep snippet focused, we assume you've added it)\n",
    "\n",
    "    rb = ReplayBuffer(capacity=50_000, rng=rng)\n",
    "    tx = make_optimizer(lr=lr, weight_decay=wd)\n",
    "    opt_state = tx.init(params)\n",
    "\n",
    "    for it in range(1, num_iters + 1):\n",
    "        # ---- Self-play ----\n",
    "        all_samples = []\n",
    "        for _ in range(games_per_iter):\n",
    "            pv_fn = make_flax_policy_value_fn(model, params)  # refresh with latest params\n",
    "            mcts.policy_value_fn = pv_fn\n",
    "            samples = self_play_game(mcts, model, params, temperature=1.0, temp_moves=6)\n",
    "            all_samples.extend(samples)\n",
    "        rb.add_many(all_samples)\n",
    "\n",
    "        # ---- Training ----\n",
    "        if len(rb) < batch_size:\n",
    "            print(f\"[iter {it}] warming up buffer ({len(rb)} samples)\")\n",
    "            continue\n",
    "\n",
    "        for step in range(train_steps):\n",
    "            feats, pis, zs = rb.sample(batch_size)\n",
    "            params, opt_state, metrics = train_step(\n",
    "                params, opt_state, model,\n",
    "                jnp.asarray(feats), jnp.asarray(pis), jnp.asarray(zs),\n",
    "                tx\n",
    "            )\n",
    "            if (step + 1) % 25 == 0:\n",
    "                print(f\"[iter {it} step {step+1}] loss={float(metrics['loss']):.4f} \"\n",
    "                      f\"pol={float(metrics['policy_loss']):.4f} val={float(metrics['value_loss']):.4f} \"\n",
    "                      f\"mae={float(metrics['value_mae']):.3f}\")\n",
    "\n",
    "        # (Optional) evaluate vs. random or a previous snapshot here\n",
    "\n",
    "    return params\n",
    "# Batched apply for many positions at once (roots or leaves)\n",
    "@jax.jit\n",
    "def model_forward_batched(params, model, feats_b: jnp.ndarray, legal_masks_b: jnp.ndarray):\n",
    "    logits, value = model.apply(params, feats_b)            # (B,9),(B,)\n",
    "    # Mask+softmax per item\n",
    "    def _per_item(l, m):\n",
    "        masked = jnp.where(m > 0.5, l, -1e9)\n",
    "        return jax.nn.softmax(masked)\n",
    "    probs = jax.vmap(_per_item)(logits, legal_masks_b)      # (B,9)\n",
    "    return probs, value\n",
    "\n",
    "def eval_many_positions(model, params, envs: List[TicTacToe]):\n",
    "    feats = np.stack([make_features(e.board, e.to_move) for e in envs], axis=0)\n",
    "    masks = []\n",
    "    for e in envs:\n",
    "        legal = e.legal_actions()\n",
    "        m = np.zeros(9, dtype=np.float32); m[legal] = 1.0\n",
    "        masks.append(m)\n",
    "    masks = np.stack(masks, 0)\n",
    "    probs_b, values_b = model_forward_batched(params, model, jnp.asarray(feats), jnp.asarray(masks))\n",
    "    # Convert to Python types\n",
    "    probs_b = np.array(probs_b); values_b = np.array(values_b)\n",
    "    # Return list of (priors_dict, value)\n",
    "    out = []\n",
    "    for i, env in enumerate(envs):\n",
    "        legal = env.legal_actions()\n",
    "        priors = {int(a): float(probs_b[i, a]) for a in legal}\n",
    "        s = sum(priors.values()) or 1.0\n",
    "        for a in list(priors.keys()): priors[a] /= s\n",
    "        out.append((priors, float(values_b[i])))\n",
    "    return out\n",
    "    \n",
    "def mcts_search_with_leaf_batching(root_env: TicTacToe, mcts: PUCT_MCTS,\n",
    "                                   model: TTTNet, params, n_simulations: int = 512, batch_size: int = 64,\n",
    "                                   temperature: float = 1.0):\n",
    "    # Prepare root\n",
    "    root = Node(key=state_to_key(root_env), to_move=root_env.to_move)\n",
    "    mcts._expand(root_env, root)\n",
    "    mcts._add_root_dirichlet_noise(root)\n",
    "    mcts.last_root = root  # keep for visit extraction\n",
    "\n",
    "    pending = []   # list of (path, leaf_env, parent_node_for_child, action_taken)\n",
    "    done = 0\n",
    "\n",
    "    while done < n_simulations:\n",
    "        # 1) Collect up to batch_size leaves (selection+expansion only)\n",
    "        batch_paths, batch_envs, batch_parents, batch_actions = [], [], [], []\n",
    "        k = min(batch_size, n_simulations - done)\n",
    "        for _ in range(k):\n",
    "            node = root\n",
    "            env = root_env.clone()\n",
    "            path = []\n",
    "            # Descend until we hit an unexpanded child or terminal\n",
    "            while True:\n",
    "                if env.is_terminal():\n",
    "                    # Terminal: back up exact outcome immediately\n",
    "                    v = env.result_from(root_env.to_move)\n",
    "                    mcts._backup(path, leaf_value=v)\n",
    "                    break\n",
    "                a = mcts._puct_select(node)\n",
    "                path.append((node, a))\n",
    "                if a not in node.children:\n",
    "                    # Expansion deferred — gather for batch eval\n",
    "                    env_next = env.step(a)\n",
    "                    batch_paths.append(path)\n",
    "                    batch_envs.append(env_next)\n",
    "                    batch_parents.append(node)\n",
    "                    batch_actions.append(a)\n",
    "                    break\n",
    "                env = env.step(a)\n",
    "                node = node.children[a]\n",
    "\n",
    "        # 2) Evaluate all leaves in one shot\n",
    "        if batch_envs:\n",
    "            evals = eval_many_positions(model, params, batch_envs)  # list of (priors, value)\n",
    "            for path, env_next, parent, a, (priors, val) in zip(batch_paths, batch_envs, batch_parents, batch_actions, evals):\n",
    "                child = Node(key=state_to_key(env_next), to_move=env_next.to_move, parent=parent, parent_action=a)\n",
    "                child.P = priors\n",
    "                parent.children[a] = child\n",
    "                mcts._backup(path, leaf_value=val, leaf_env_to_move=env_next.to_move, root_player=root_env.to_move)\n",
    "\n",
    "        done += k\n",
    "\n",
    "    # 3) Pick move from visit counts\n",
    "    return mcts._select_action_from_visits(root, temperature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8514990f-55e5-4a8b-9148-fb8e7c39b142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6c39c03c-0d89-4553-957c-f45ea1a4ccc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "36e80958-3f62-4269-a81a-d8bdc319c05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 1 step 25] loss=173611104.0000 pol=173611104.0000 val=0.1143 mae=0.247\n",
      "[iter 1 step 50] loss=152777776.0000 pol=152777776.0000 val=0.1409 mae=0.249\n",
      "[iter 2 step 25] loss=52083320.0000 pol=52083320.0000 val=1.5223 mae=1.029\n",
      "[iter 2 step 50] loss=152777600.0000 pol=152777600.0000 val=1.0625 mae=0.875\n",
      "[iter 3 step 25] loss=114582624.0000 pol=114582624.0000 val=1.2500 mae=0.875\n",
      "[iter 3 step 50] loss=152773856.0000 pol=152773856.0000 val=1.6875 mae=1.125\n",
      "[iter 4 step 25] loss=128464432.0000 pol=128464432.0000 val=1.5312 mae=1.031\n",
      "[iter 4 step 50] loss=83320024.0000 pol=83320024.0000 val=1.3750 mae=0.938\n",
      "[iter 5 step 25] loss=100660552.0000 pol=100660552.0000 val=1.0000 mae=0.812\n",
      "[iter 5 step 50] loss=48572440.0000 pol=48572440.0000 val=1.2500 mae=0.938\n"
     ]
    }
   ],
   "source": [
    "params = train_loop(\n",
    "    num_iters=5,        # number of outer loops (increase later)\n",
    "    games_per_iter=8,   # how many self-play games per iteration\n",
    "    batch_size=32,      # training batch size\n",
    "    train_steps=50,     # SGD updates per iteration\n",
    "    sims_per_move=100   # MCTS simulations per move\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fb6791-e951-4246-bd7c-312e4dafa1c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
